


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Program Listing for File module.h &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/api/program_listing_file_torch_csrc_jit_script_module.h.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="File modules.h" href="file_torch_csrc_api_include_torch_nn_modules.h.html" />
    <link rel="prev" title="File module.h" href="file_torch_csrc_jit_script_module.h.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installing.html">Installing C++ Distributions of PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../frontend.html">The C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing to PyTorch</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="library_root.html">Library API</a></li>
</ul>
<p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notes/faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_basics.html">Tensor Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_creation.html">Tensor Creation API</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="library_root.html">Library API</a> &gt;</li>
        
          <li><a href="file_torch_csrc_jit_script_module.h.html">File module.h</a> &gt;</li>
        
      <li>Program Listing for File module.h</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            
              <!-- User defined GitHub URL -->
              <a href="https://github.com/pytorch/pytorch" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="program-listing-for-file-module-h">
<span id="program-listing-file-torch-csrc-jit-script-module-h"></span><h1>Program Listing for File module.h<a class="headerlink" href="#program-listing-for-file-module-h" title="Permalink to this headline">¶</a></h1>
<p>↰ <a class="reference internal" href="file_torch_csrc_jit_script_module.h.html#file-torch-csrc-jit-script-module-h"><span class="std std-ref">Return to documentation for file</span></a> (<code class="docutils literal notranslate"><span class="pre">torch/csrc/jit/script/module.h</span></code>)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>#pragma once
#include &lt;torch/csrc/autograd/variable.h&gt;
#include &lt;torch/csrc/autograd/generated/variable_factories.h&gt;
#include &lt;torch/csrc/jit/argument_spec.h&gt;
#include &lt;c10/util/Exception.h&gt;
#include &lt;torch/csrc/jit/graph_executor.h&gt;
#include &lt;torch/csrc/jit/ir.h&gt;
#include &lt;torch/csrc/jit/named_value.h&gt;
#include &lt;torch/csrc/jit/passes/shape_analysis.h&gt;
#include &lt;torch/csrc/jit/source_range.h&gt;

#include &lt;torch/csrc/WindowsTorchApiMacro.h&gt;
#include &lt;torch/csrc/api/include/torch/ordered_dict.h&gt;
#include &lt;torch/csrc/utils/memory.h&gt;

#include &lt;ATen/core/function_schema.h&gt;
#include &lt;c10/util/ArrayRef.h&gt;
#include &lt;c10/util/Optional.h&gt;

#include &lt;functional&gt;
#include &lt;memory&gt;
#include &lt;mutex&gt;
#include &lt;ostream&gt;
#include &lt;string&gt;
#include &lt;unordered_map&gt;
#include &lt;vector&gt;

// This file contains classes which assist in desugaring Python style
// modules and their methods into flattened graphs which don&#39;t have any
// function calls.

namespace torch {
namespace jit {
namespace script {

using ::c10::Argument;
using ::c10::FunctionSchema;
// Map which stores filename to content.
using ExtraFilesMap = std::unordered_map&lt;std::string, std::string&gt;;

// A method in a module, e.g. f in:
//
// class M(ScriptModule):
//   @script_method
//   def f(self, x):
//     ...
// Note: because Method/Module are exposed to python these
// classes use python method naming conventions

struct Module;

using ModuleLookup = std::function&lt;std::shared_ptr&lt;Module&gt;(
    const std::vector&lt;std::string&gt;&amp;)&gt;;

struct Method {
  Method(
      Module* owner,
      std::string name,
      bool optimize,
      std::shared_ptr&lt;Graph&gt; graph,
      std::vector&lt;IValue*&gt; initial_members,
      std::function&lt;void(Method&amp;)&gt; method_creator)
      : owner_(owner),
        name_(std::move(name)),
        graph_(std::move(graph)),
        optimize(optimize),
        initial_ivalues_(std::move(initial_members)),
        method_creator(std::move(method_creator)) {
    AT_ASSERT(graph_-&gt;inputs().size() &gt;= initial_ivalues_.size());
    int i = graph_-&gt;inputs().size() - initial_ivalues_.size();
    for (auto member : initial_ivalues_) {
      initial_ivalue_index[member] = i++;
    }
  }

  void run(Stack&amp; stack) {
    for (auto input : initial_ivalues_) {
      push(stack, *input);
    }
    get_executor().run(stack);
  }

  void run(Stack&amp;&amp; stack) {
    run(stack);
  }

  IValue operator()(std::vector&lt;IValue&gt; stack) {
    checkInputsAgainstSchema(stack);
    run(stack);
    return stack.front();
  }

  std::shared_ptr&lt;Graph&gt; graph_for(Stack inputs) {
    for (auto tp : initial_ivalues_) {
      inputs.emplace_back(*tp);
    }
    return get_executor().graphFor(inputs);
  }
  TORCH_API std::shared_ptr&lt;Graph&gt; graph() const {
    return graph_;
  }

  TORCH_API const std::string&amp; name() const {
    return name_;
  }
  // emit a function call by inlining the callees Graph into this one
  // adding any extra parameters necessary to do this call

  // defined here to keep details of member_input handling confined to this
  // class
  Value* emit_call_to(
      const SourceRange&amp; loc,
      Method&amp; callee,
      ArrayRef&lt;NamedValue&gt; args,
      ArrayRef&lt;NamedValue&gt; kwargs);

  // if this isn&#39;t yet defined, run its method_creator function
  TORCH_API void ensure_defined();

  size_t num_inputs() const {
    return graph()-&gt;inputs().size() - initial_ivalues_.size();
  }
  TORCH_API Value* get_or_add_parameter(IValue* slot) {
    AT_ASSERT(slot-&gt;isTensor());
    return get_or_add_attribute(TensorType::get(), slot);
  }

  TORCH_API Value* get_or_add_attribute(TypePtr type, IValue* slot) {
    auto it = initial_ivalue_index.find(slot);
    if (it != initial_ivalue_index.end()) {
      return graph()-&gt;inputs().at(it-&gt;second);
    }
    initial_ivalues_.push_back(slot);
    initial_ivalue_index[slot] = graph()-&gt;inputs().size();
    return graph()-&gt;addInput()-&gt;setType(type);
  }

  std::shared_ptr&lt;Graph&gt; propagate_shapes(
      std::vector&lt;at::Tensor&gt; inputs,
      bool with_grad = false) {
    auto retval = graph_-&gt;copy();
    Stack stack;
    stack.reserve(inputs.size() + initial_ivalues_.size());
    for (at::Tensor&amp; i : inputs) {
      stack.emplace_back(std::move(i));
    }
    for (IValue* inp : initial_ivalues_) {
      stack.push_back(*inp);
    }
    const auto size = stack.size();
    setInputTypes(*retval, ArgumentSpec(with_grad, stack, size));
    PropagateInputShapes(retval);
    return retval;
  }

  std::shared_ptr&lt;Graph&gt; propagate_and_assign_input_and_output_shapes(
      std::vector&lt;at::Tensor&gt; inputs,
      std::vector&lt;at::Tensor&gt; outputs,
      bool with_grad = false,
      bool propagate = true) {
    auto retval = graph_-&gt;copy();
    for (auto inp : initial_ivalues_) {
      if (inp-&gt;isTensor()) {
        inputs.push_back(inp-&gt;toTensor());
      }
    }
    if (propagate) {
      setInputTypes(
          *retval,
          ArgumentSpec(with_grad, fmap&lt;IValue&gt;(inputs), inputs.size()));
      PropagateInputShapes(retval);
    }
    AT_ASSERT(retval-&gt;inputs().size() == inputs.size());
    for (size_t i = 0; i &lt; retval-&gt;inputs().size(); ++i) {
      auto scalar_type = inputs[i].scalar_type();
      auto sizes = inputs[i].sizes();
      auto type =
          torch::jit::CompleteTensorType::create(scalar_type, at::kCPU, sizes);
      retval-&gt;inputs()[i]-&gt;setType(type);
    }
    at::ArrayRef&lt;Value*&gt; output_values = retval-&gt;outputs();
    // patch this to still work if we are returning a tuple of multiple values
    if (output_values.at(0)-&gt;type()-&gt;kind() == TupleType::Kind) {
      AT_ASSERT(output_values.at(0)-&gt;node()-&gt;kind() == prim::TupleConstruct);
      output_values = output_values.at(0)-&gt;node()-&gt;inputs();
    }
    AT_ASSERT(output_values.size() == outputs.size());
    for (size_t i = 0; i &lt; retval-&gt;outputs().size(); ++i) {
      auto scalar_type = outputs[i].scalar_type();
      auto sizes = outputs[i].sizes();
      auto type =
          torch::jit::CompleteTensorType::create(scalar_type, at::kCPU, sizes);
      output_values[i]-&gt;setType(type);
    }
    return retval;
  }

  const std::vector&lt;IValue*&gt;&amp; initial_ivalues() const {
    return initial_ivalues_;
  }

  Method&amp; setSchema(FunctionSchema schema_) {
    schema = make_unique&lt;FunctionSchema&gt;(std::move(schema_));
    return *this;
  }

  TORCH_API const FunctionSchema&amp; getSchema() const {
    if (schema == nullptr) {
      schema = make_unique&lt;FunctionSchema&gt;(defaultSchemaFor(*this));
    }
    return *schema;
  }

  std::string pretty_print_schema() const {
    AT_ASSERT(schema);
    std::stringstream ss;
    ss &lt;&lt; *schema;
    return ss.str();
  }

  GraphExecutorState getDebugState() {
    return get_executor().getDebugState();
  }

  void debugDisableAutodiffSubgraphInlining() {
    return get_executor().debugDisableAutodiffSubgraphInlining();
  }

  bool is_optimized() const {
    return optimize;
  }

  // the module that contains this method.
  Module&amp; owner() const {
    return *owner_;
  }

  void check_single_output() {
    AT_CHECK(
        graph()-&gt;outputs().size() == 1,
        &quot;Method (but not graphs in general) require a single output. Use None/Tuple for 0 or 2+ outputs&quot;);
  }

 private:
  static FunctionSchema defaultSchemaFor(const Method&amp; method) {
    std::vector&lt;Argument&gt; args;
    std::vector&lt;Argument&gt; returns;
    Graph&amp; g = *method.graph();
    size_t num_inputs = method.num_inputs();
    for (size_t i = 0; i &lt; num_inputs; ++i) {
      const Value* v = g.inputs().at(i);
      std::string name = v-&gt;hasUniqueName() ? v-&gt;uniqueNameBase()
                                            : (&quot;argument_&quot; + std::to_string(i));
      args.emplace_back(std::move(name), unshapedType(g.inputs()[i]-&gt;type()));
    }
    for (size_t i = 0; i &lt; g.outputs().size(); ++i) {
      returns.emplace_back(&quot;&quot;, unshapedType(g.outputs()[i]-&gt;type()));
    }
    return {method.name(), &quot;&quot;, std::move(args), std::move(returns)};
  }

  GraphExecutor&amp; get_executor() {
    std::call_once(executor_init, [&amp;] {
      check_single_output();
      executor = GraphExecutor(graph(), optimize);
    });
    return executor;
  }

  void checkInputsAgainstSchema(std::vector&lt;IValue&gt;&amp; inputs) {
    const auto&amp; schema = getSchema();
    // Do we have more inputs than the schema accepts?
    AT_CHECK(
        inputs.size() &lt;= schema.arguments().size(),
        &quot;Expected at most &quot;,
        schema.arguments().size(),
        &quot; argument(s) for operator &#39;&quot;,
        schema.name(),
        &quot;&#39;, but received &quot;,
        inputs.size(),
        &quot; argument(s). Declaration: &quot;,
        schema);

    for (size_t pos = 0; pos &lt; schema.arguments().size(); ++pos) {
      const auto&amp; argument = schema.arguments()[pos];
      if (pos &lt; inputs.size()) {
        if (!isSubvalueOf(inputs[pos], argument.type())) {
          AT_ERROR(
            &quot;Expected value of type &quot;,
            *argument.type(),
            &quot; for argument &#39;&quot;,
            argument.name(),
            &quot;&#39; in position &quot;,
            pos,
            &quot;, but instead got value of type &quot;,
            attemptToRecoverType(inputs[pos])-&gt;str(),
            &quot;. Declaration: &quot;,
            schema);
        }
      } else if (argument.default_value()) {
        inputs.push_back(*argument.default_value());
      } else {
        AT_ERROR(
            schema.name(),
            &quot;() is missing value for argument &#39;&quot;,
            argument.name(),
            &quot;&#39;. Declaration: &quot;,
            schema);
      }
    }
  }

  // Methods are uniqued onwed by a single module. This raw pointer allows
  // looking up the module.
  Module* owner_;

  std::string name_;
  std::shared_ptr&lt;Graph&gt; graph_; // for debugging and for inlining
  bool optimize;

  GraphExecutor executor; // for execution
  // initial_ivalues are a list of additional arguments appended to graph
  // that are inputs that come from the members of the Module or its submodules.
  // each is a pointer to a slot in the module that owns this parameter
  // parameters and submodules can only be _added_ to script Modules to ensure
  // these pointers always stay valid
  std::vector&lt;IValue*&gt; initial_ivalues_;

  // map from a IValue* in initial_ivalues to the offset it appears at
  // in graph. used to accelerate get_or_add_parameter
  std::unordered_map&lt;IValue*, size_t&gt; initial_ivalue_index;

  // TODO: support that case where we allow _writes_ to parameters from
  // compiled functions.
  // This requires more sophisticated tracking of ssa values in Graphs so that
  // stores to all modules can be lifted to the end of a graph execution.
  // It also adds more complexity to adding actual module invocations
  // to the executor, so currently it is not done.
  // std::vector&lt;at::Tensor*&gt; member_outputs;

  std::once_flag executor_init;

  // an optional function that actually creates the method when
  // emit_call_to(this,...) is first called. this is used by the compiler so
  // that it can construct methods out of order
  std::function&lt;void(Method&amp;)&gt; method_creator;

  // if absent, then we generate a default schema based on the graph
  // mutable because getSchema caches the default schema if one is requested
  // before a call to setSchema
  mutable std::unique_ptr&lt;FunctionSchema&gt; schema;
};

struct Module;

struct NamedModule {
  std::string name;
  std::shared_ptr&lt;Module&gt; module;
};

struct NamedIValue {
  NamedIValue(std::string name, TypePtr type, IValue ivalue)
      : name_(name),
        type(type),
        ivalue(torch::make_unique&lt;IValue&gt;(std::move(ivalue))) {}

  IValue* slot() const {
    return ivalue.get();
  }
  const std::string name_;
  const TypePtr type;
  std::unique_ptr&lt;IValue&gt; ivalue;
};

struct Module {
  TH_DISALLOW_COPY_AND_ASSIGN(Module);
  Module()
      : modules(&quot;Module&quot;),
        parameters(&quot;Parameter&quot;),
        attributes(&quot;Attributes&quot;),
        methods(&quot;Method&quot;),
        optimize(true) {}

  // note this doesn&#39;t change the flags of existing methods just ones
  // added afterward.
  void set_optimized(bool o) {
    optimize = o;
  }

  bool is_optimized() const {
    return optimize;
  }

  IValue forward(std::vector&lt;IValue&gt; inputs) {
    return get_method(&quot;forward&quot;)(std::move(inputs));
  }

  void register_buffer(const std::string&amp; name, autograd::Variable v) {
    if (auto b = attributes.find(name)) {
      AT_ASSERT(b-&gt;type-&gt;isSubtypeOf(TensorType::get()));
      *b-&gt;slot() = v;
      return;
    }
    attributes.insert(name, NamedIValue(name, TensorType::get(), std::move(v)));
  }
  void register_parameter(
      const std::string&amp; name,
      autograd::Variable v,
      bool is_buffer) {
    if (is_buffer) {
      register_buffer(name, std::move(v));
      return;
    }
    if (auto p = parameters.find(name)) {
      *p-&gt;slot() = v;
      return;
    }
    parameters.insert(name, NamedIValue(name, TensorType::get(), std::move(v)));
  }
  void register_attribute(
      const std::string&amp; name,
      const TypePtr type,
      IValue ivalue) {
    attributes.insert(name, NamedIValue(name, type, ivalue));
  }
  void register_module(
      const std::string&amp; name,
      std::shared_ptr&lt;Module&gt; module) {
    modules.insert(name, {name, std::move(module)});
  }

  Method&amp; create_method(
      const std::string&amp; name,
      std::shared_ptr&lt;Graph&gt; graph,
      std::vector&lt;IValue*&gt; member_inputs) {
    AT_ASSERT(graph);
    std::unique_ptr&lt;Method&gt; method(new Method(
        this,
        name,
        optimize,
        std::move(graph),
        std::move(member_inputs),
        nullptr));
    return *methods.insert(name, std::move(method));
  }

  Method&amp; create_method(
      const std::string&amp; name,
      std::function&lt;void(Method&amp;)&gt; creator) {
    std::unique_ptr&lt;Method&gt; method(new Method(
        this,
        name,
        optimize,
        std::make_shared&lt;Graph&gt;(),
        {},
        std::move(creator)));
    return *methods.insert(name, std::move(method));
  }

  IValue* parameter_slot(const std::string&amp; name) const {
    return parameters[name].slot();
  }

  void set_parameter(const std::string&amp; name, at::Tensor v) {
    *parameter_slot(name) = std::move(v);
  }

  autograd::Variable get_parameter(const std::string&amp; name) const {
    return autograd::as_variable_ref(parameter_slot(name)-&gt;toTensor());
  }
  autograd::Variable get_buffer(const std::string&amp; name) const {
    return autograd::as_variable_ref(attributes.find(name)-&gt;slot()-&gt;toTensor());
  }

  // each module owns its method. The reference returned here
  // is guarenteed to stay valid until this module has been destroyed
  Method&amp; get_method(const std::string&amp; name) const {
    return *methods[name];
  }

  std::shared_ptr&lt;Module&gt; get_module(const std::string&amp; name) const {
    return modules[name].module;
  }

  const torch::OrderedDict&lt;std::string, NamedModule&gt;&amp; get_modules() const {
    return modules;
  }
  const torch::OrderedDict&lt;std::string, NamedIValue&gt;&amp; get_parameters()
      const {
    return parameters;
  }
  const torch::OrderedDict&lt;std::string, NamedIValue&gt;&amp; get_attributes()
      const {
    return attributes;
  }
  const torch::OrderedDict&lt;std::string, std::unique_ptr&lt;Method&gt;&gt;&amp; get_methods()
      const {
    return methods;
  }

  NamedIValue* find_parameter(const std::string&amp; name) {
    return parameters.find(name);
  }
  NamedIValue* find_attribute(const std::string&amp; name) {
    return attributes.find(name);
  }
  NamedIValue* find_buffer(const std::string&amp; name) {
    auto b = attributes.find(name);
    if (b &amp;&amp; b-&gt;type-&gt;isSubtypeOf(TensorType::get())) {
      return b;
    }
    return nullptr;
  }
  NamedModule* find_module(const std::string&amp; name) {
    return modules.find(name);
  }
  Method* find_method(const std::string&amp; name) {
    if (auto* pm = methods.find(name)) {
      return pm-&gt;get();
    }
    return nullptr;
  }
  void apply(std::function&lt;void(Module&amp;)&gt; fn) {
    for (auto&amp; submod : get_modules()) {
      submod.value().module-&gt;apply(fn);
    }
    fn(*this);
  }
  void train(bool on = true) {
    for (auto&amp; submod : get_modules()) {
      submod-&gt;module-&gt;train(on);
    }
    register_buffer(&quot;training&quot;, torch::tensor(on ? 1 : 0, at::kLong));
  }
  void eval() {
    train(/*on=*/false);
  }
  bool is_training() {
    if (auto p = find_buffer(&quot;training&quot;)) {
      return p-&gt;slot()-&gt;toTensor().item&lt;int64_t&gt;() == 1;
    }
    // We are in training mode by default
    return true;
  }

  TORCH_API void to(
      at::Device device,
      at::ScalarType dtype,
      bool non_blocking = false);

  TORCH_API void to(at::ScalarType dtype, bool non_blocking = false);

  TORCH_API void to(at::Device device, bool non_blocking = false);

  template &lt;typename... Types&gt;
  IValue run_method(const std::string&amp; method_name, Types&amp;&amp;... args) {
    return get_method(method_name)({IValue(std::forward&lt;Types&gt;(args))...});
  }

  void save(
      std::ostream&amp; out,
      const ExtraFilesMap&amp; extra_files = ExtraFilesMap());

  void save(
      const std::string&amp; filename,
      const ExtraFilesMap&amp; extra_files = ExtraFilesMap());

  void copy_into(
      ModuleLookup module_lookup,
      // parameter_remap is needed when a parent module uses a parameter of a
      // submodule
      std::unordered_map&lt;IValue*, IValue*&gt;&amp; parameter_remap,
      std::vector&lt;std::string&gt; names = {}) const {
    auto curr = module_lookup(names);
    for (auto&amp; kv : parameters) {
      curr-&gt;register_parameter(
          kv.key(),
          kv.value().slot()-&gt;toTensor(),
          /*is_buffer=*/false);
      parameter_remap[kv.value().slot()] = curr-&gt;parameter_slot(kv.key());
    }
    for (auto&amp; kv : attributes) {
      if (!kv.value().type-&gt;isSubtypeOf(TensorType::get())) {
        continue;
      }
      curr-&gt;register_buffer(
          kv.key(),
          kv.value().slot()-&gt;toTensor());
      parameter_remap[kv.value().slot()] = curr-&gt;find_buffer(kv.key())-&gt;slot();
    }
    for (auto&amp; kv : modules) {
      names.push_back(kv.key());
      // Submodules must be translated first, otherwise parameter_remap entries
      // will not be filled in for methods of this module.
      kv.value().module-&gt;copy_into(module_lookup, parameter_remap, names);
      names.pop_back();
    }
    for (auto&amp; kv : methods) {
      std::vector&lt;IValue*&gt; initial_ivalues;
      for (auto&amp; p : kv.value()-&gt;initial_ivalues()) {
        initial_ivalues.push_back(parameter_remap.at(p));
      }
      curr-&gt;create_method(
          kv.key(), kv.value()-&gt;graph()-&gt;copy(), initial_ivalues);
    }
  }

 private:
  void to_impl(
      const c10::optional&lt;at::Device&gt;&amp; device,
      const c10::optional&lt;at::ScalarType&gt;&amp; dtype,
      bool non_blocking);

  // invariant: to ensure initial_ivalues of Methods stay valid,
  // it is only legal to _add_ new modules and parameters.
  // removing them will allow initial_ivalues to point to invalid parameters
  // no such restriction exists for methods
  torch::OrderedDict&lt;std::string, NamedModule&gt; modules;
  torch::OrderedDict&lt;std::string, NamedIValue&gt; parameters;
  torch::OrderedDict&lt;std::string, NamedIValue&gt; attributes;
  torch::OrderedDict&lt;std::string, std::unique_ptr&lt;Method&gt;&gt; methods;
  bool optimize;
};

// returns nullptr and fills in failure_messages if the callee does not
// match the functions schema
Value* try_emit_call_to(
    Graph&amp; graph,
    const SourceRange&amp; loc,
    Method&amp; callee,
    c10::optional&lt;NamedValue&gt; self,
    ArrayRef&lt;NamedValue&gt; args,
    ArrayRef&lt;NamedValue&gt; kwargs,
    std::stringstream&amp; failure_messages,
    // when callee uses no parameters (e.g. it is a function in a compilation
    // unit, and not a method), then nullptr can be passed as caller.
    Method* caller,
    bool conv_tensors_to_nums);
} // namespace script
} // namespace jit
} // namespace torch
</pre></div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="file_torch_csrc_api_include_torch_nn_modules.h.html" class="btn btn-neutral float-right" title="File modules.h" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="file_torch_csrc_jit_script_module.h.html" class="btn btn-neutral" title="File module.h" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Program Listing for File module.h</a></li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../_static/jquery.js"></script>
         <script type="text/javascript" src="../_static/underscore.js"></script>
         <script type="text/javascript" src="../_static/doctools.js"></script>
         <script type="text/javascript" src="../_static/language_data.js"></script>
         <script type="text/javascript" src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
         <script type="text/javascript" src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>