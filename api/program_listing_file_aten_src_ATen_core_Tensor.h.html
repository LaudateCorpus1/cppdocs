


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Program Listing for File Tensor.h &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/api/program_listing_file_aten_src_ATen_core_Tensor.h.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="File tensor.h" href="file_torch_csrc_api_include_torch_data_datasets_tensor.h.html" />
    <link rel="prev" title="File Tensor.h" href="file_aten_src_ATen_core_Tensor.h.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installing.html">Installing C++ Distributions of PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../frontend.html">The C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing to PyTorch</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="library_root.html">Library API</a></li>
</ul>
<p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notes/faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_basics.html">Tensor Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_creation.html">Tensor Creation API</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="library_root.html">Library API</a> &gt;</li>
        
          <li><a href="file_aten_src_ATen_core_Tensor.h.html">File Tensor.h</a> &gt;</li>
        
      <li>Program Listing for File Tensor.h</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            
              <!-- User defined GitHub URL -->
              <a href="https://github.com/pytorch/pytorch" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="program-listing-for-file-tensor-h">
<span id="program-listing-file-aten-src-aten-core-tensor-h"></span><h1>Program Listing for File Tensor.h<a class="headerlink" href="#program-listing-for-file-tensor-h" title="Permalink to this headline">¶</a></h1>
<p>↰ <a class="reference internal" href="file_aten_src_ATen_core_Tensor.h.html#file-aten-src-aten-core-tensor-h"><span class="std std-ref">Return to documentation for file</span></a> (<code class="docutils literal notranslate"><span class="pre">aten/src/ATen/core/Tensor.h</span></code>)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>#pragma once

#include &lt;c10/Device.h&gt;
#include &lt;c10/core/Layout.h&gt;
#include &lt;c10/core/Scalar.h&gt;
#include &lt;c10/core/ScalarType.h&gt;
#include &lt;ATen/core/SparseTensorRef.h&gt;
#include &lt;c10/core/Storage.h&gt;
#include &lt;ATen/core/TensorAccessor.h&gt;
#include &lt;c10/core/TensorImpl.h&gt;
#include &lt;c10/core/UndefinedTensorImpl.h&gt;
#include &lt;c10/util/Exception.h&gt;
#include &lt;c10/util/Optional.h&gt;
#include &lt;ATen/core/LegacyTypeDispatch.h&gt;

namespace c10{
struct TensorOptions;
}
namespace at {
struct Generator;
struct Type;
class Tensor;
} // namespace at

namespace at {

class Tensor;
using TensorList = ArrayRef&lt;Tensor&gt;;

// Tensor is a &quot;generic&quot; object holding a pointer to the underlying TensorImpl object, which
// has an embedded reference count. In this way, Tensor is similar to boost::intrusive_ptr.
//
// For example:
//
// void func(Tensor a) {
//   Tensor b = a;
//   ...
// }
//
// In this example, when we say Tensor b = a, we are creating a new object that points to the
// same underlying TensorImpl, and bumps its reference count. When b goes out of scope, the
// destructor decrements the reference count by calling release() on the TensorImpl it points to.
// The existing constructors, operator overloads, etc. take care to implement the correct semantics.
//
// Note that Tensor can also be NULL, i.e. it is not associated with any underlying TensorImpl, and
// special care must be taken to handle this.
class CAFFE2_API Tensor {
public:
  Tensor(){};
  Tensor(c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt; tensor_impl)
      : impl_(std::move(tensor_impl)) {
    if (impl_.get() == nullptr) {
      throw std::runtime_error(&quot;TensorBaseImpl with nullptr not supported&quot;);
    }
  }

  Tensor(const Tensor&amp;) = default;
  Tensor(Tensor&amp;&amp;) = default;

  int64_t dim() const {
    return impl_-&gt;dim();
  }
  int64_t storage_offset() const {
    return impl_-&gt;storage_offset();
  }

  TensorImpl * unsafeGetTensorImpl() const {
    return impl_.get();
  }
  TensorImpl * unsafeReleaseTensorImpl() {
    return impl_.release();
  }
  const c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt;&amp; getIntrusivePtr() const {
    return impl_;
  }

  bool defined() const {
    return impl_;
  }

  void reset() {
    impl_.reset();
  }

  // The following overloads are very intruiging.  Consider the following
  // program:
  //
  //    x[1] = 3;
  //
  // We would expect that the first entry of x is written to 3.  But how can we
  // actually achieve this?  x[1] evaluates to a tensor...
  //
  // The answer is, using a ref-qualifier.  x[1] is an rvalue, which cannot be
  // (profitably) assigned to in the traditional sense, so we overload
  // assignment to mean, &quot;Actually, copy 3 into the tensor data.&quot;  This is done
  // with an rvalue-reference ref-qualified overload (the methods with &amp;&amp; at the
  // end of their type.)
  //
  // There&#39;s one more fly in the ointment: We also want
  //
  //    Tensor x = y;
  //
  // to work, and we want it NOT to copy.  So we need a traditional operator=
  // overload.  But we MUST specify a mutable lvalue ref-qualifier, to
  // disambiguate the traditional overload from the rvalue-reference
  // ref-qualified overload.  Otherwise, it will be ambiguous, because
  // a non ref-qualified method is eligible for all situations.

  // Unfortunately, we have to write these constructors out manually
  // to work around an MSVC bug:
  //    error C2580: &#39;at::Tensor &amp;at::Tensor::operator =(const at::Tensor &amp;) &amp;&#39;:
  //    multiple versions of a defaulted special member functions are not allowed
  // Tensor&amp; operator=(const Tensor&amp;) &amp; = default;
  // Tensor&amp; operator=(Tensor&amp;&amp;) &amp; = default;
  Tensor&amp; operator=(const Tensor&amp; x) &amp; {
    impl_ = x.impl_;
    return *this;
  }
  Tensor&amp; operator=(Tensor&amp;&amp; x) &amp; {
    impl_ = std::move(x.impl_);
    return *this;
  }

  Tensor&amp; operator=(Scalar v) &amp;&amp;;
  Tensor&amp; operator=(const Tensor&amp;) &amp;&amp;;
  Tensor&amp; operator=(Tensor&amp;&amp;) &amp;&amp;;

  bool is_same(const Tensor&amp; other) const noexcept {
    return impl_ == other.impl_;
  }
  size_t use_count() const noexcept {
    return impl_.use_count();
  }
  size_t weak_use_count() const noexcept {
    return impl_.weak_use_count();
  }

  const char * toString() const;

  IntList sizes() const {
    return impl_-&gt;sizes();
  }
  IntList strides() const {
    return impl_-&gt;strides();
  }
  int64_t ndimension() const {
    return dim();
  }
  bool is_contiguous() const {
    return impl_-&gt;is_contiguous();
  }
  Type &amp; type() const {
    return legacyTensorType(*impl_);
  }
  TensorTypeId type_id() const {
    return impl_-&gt;type_id();
  }
  ScalarType scalar_type() const {
    return typeMetaToScalarType(impl_-&gt;dtype());
  }
  const Storage&amp; storage() const {
    return impl_-&gt;storage();
  }
  bool is_alias_of(const at::Tensor&amp; other) const{
    return impl_-&gt;storage().is_alias_of(other.storage());
  }
  Tensor toType(const Type &amp; t, bool non_blocking=false) const;
  Tensor &amp; copy_(const Tensor &amp; src, bool non_blocking=false);
  Tensor toType(ScalarType t) const;
  Tensor toBackend(Backend b) const;

  bool is_variable() const noexcept;

  Layout layout() const noexcept;

  caffe2::TypeMeta dtype() const noexcept;

  Device device() const;

  int64_t get_device() const;

  bool is_cuda() const;

  bool is_hip() const;

  bool is_sparse() const;

  TensorOptions options() const;

  template&lt;typename T&gt;
  T * data() const;

  template &lt;typename T&gt;
  T item() const;

  // Purposely not defined here to avoid inlining
  void print() const;

  // Return a `TensorAccessor` for CPU `Tensor`s. You have to specify scalar type and
  // dimension.
  template&lt;typename T, size_t N&gt;
  TensorAccessor&lt;T,N&gt; accessor() const&amp; {
    static_assert(N &gt; 0, &quot;accessor is used for indexing tensor, for scalars use *data&lt;T&gt;()&quot;);
    AT_CHECK(dim() == N, &quot;expected &quot;, N, &quot; dims but tensor has &quot;, dim());
    return TensorAccessor&lt;T,N&gt;(data&lt;T&gt;(),sizes().data(),strides().data());
  }
  template&lt;typename T, size_t N&gt;
  TensorAccessor&lt;T,N&gt; accessor() &amp;&amp; = delete;

  // Return a `PackedTensorAccessor` for CUDA `Tensor`s. You have to specify scalar type and
  // dimension. You can optionally specify RestrictPtrTraits as a template parameter to
  // cast the data pointer to a __restrict__ pointer.
  // In order to use this, your CUDA kernel has to take a corresponding PackedTensorAccessor
  // as an argument.
  template&lt;typename T, size_t N, template &lt;typename U&gt; class PtrTraits = DefaultPtrTraits, typename index_t = int64_t&gt;
  PackedTensorAccessor&lt;T,N,PtrTraits,index_t&gt; packed_accessor() const&amp; {
    static_assert(N &gt; 0, &quot;accessor is used for indexing tensor, for scalars use *data&lt;T&gt;()&quot;);
    AT_CHECK(dim() == N, &quot;expected &quot;, N, &quot; dims but tensor has &quot;, dim());
    return PackedTensorAccessor&lt;T,N,PtrTraits,index_t&gt;(static_cast&lt;typename PtrTraits&lt;T&gt;::PtrType&gt;(data&lt;T&gt;()),sizes().data(),strides().data());
  }
  template&lt;typename T, size_t N,  template &lt;typename U&gt; class PtrTraits = DefaultPtrTraits, typename index_t = int64_t&gt;
  PackedTensorAccessor&lt;T,N&gt; packed_accessor() &amp;&amp; = delete;

  Tensor operator-() const;
  Tensor&amp; operator+=(const Tensor &amp; other);
  Tensor&amp; operator+=(Scalar other);
  Tensor&amp; operator-=(const Tensor &amp; other);
  Tensor&amp; operator-=(Scalar other);
  Tensor&amp; operator*=(const Tensor &amp; other);
  Tensor&amp; operator*=(Scalar other);
  Tensor&amp; operator/=(const Tensor &amp; other);
  Tensor&amp; operator/=(Scalar other);
  Tensor operator[](Scalar index) const;
  Tensor operator[](Tensor index) const;
  Tensor operator[](int64_t index) const;

  Tensor cpu() const;
  Tensor cuda() const;
  Tensor hip() const;

  // ~~~~~ Autograd API ~~~~~

  Tensor&amp; set_requires_grad(bool requires_grad) {
    impl_-&gt;set_requires_grad(requires_grad);
    return *this;
  }
  bool requires_grad() const {
    return impl_-&gt;requires_grad();
  }

  Tensor&amp; grad() {
    return impl_-&gt;grad();
  }
  const Tensor&amp; grad() const {
    return impl_-&gt;grad();
  }

  void set_data(Tensor new_data);

  void backward(
      c10::optional&lt;Tensor&gt; gradient = c10::nullopt,
      bool keep_graph = false,
      bool create_graph = false);

  // STOP.  Thinking of adding a method here, which only makes use
  // of other ATen methods?  Define it in native_functions.yaml.

  //example
  //Tensor * add(Tensor &amp; b);
  Tensor abs() const;
  Tensor &amp; abs_();
  Tensor acos() const;
  Tensor &amp; acos_();
  Tensor add(const Tensor &amp; other, Scalar alpha=1) const;
  Tensor &amp; add_(const Tensor &amp; other, Scalar alpha=1);
  Tensor add(Scalar other, Scalar alpha=1) const;
  Tensor &amp; add_(Scalar other, Scalar alpha=1);
  Tensor addmv(const Tensor &amp; mat, const Tensor &amp; vec, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; addmv_(const Tensor &amp; mat, const Tensor &amp; vec, Scalar beta=1, Scalar alpha=1);
  Tensor addr(const Tensor &amp; vec1, const Tensor &amp; vec2, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; addr_(const Tensor &amp; vec1, const Tensor &amp; vec2, Scalar beta=1, Scalar alpha=1);
  Tensor all(int64_t dim, bool keepdim=false) const;
  bool allclose(const Tensor &amp; other, double rtol=1e-05, double atol=1e-08, bool equal_nan=false) const;
  Tensor any(int64_t dim, bool keepdim=false) const;
  Tensor argmax(int64_t dim, bool keepdim=false) const;
  Tensor argmax() const;
  Tensor argmin(int64_t dim, bool keepdim=false) const;
  Tensor argmin() const;
  Tensor as_strided(IntList size, IntList stride) const;
  Tensor &amp; as_strided_(IntList size, IntList stride);
  Tensor as_strided(IntList size, IntList stride, int64_t storage_offset) const;
  Tensor &amp; as_strided_(IntList size, IntList stride, int64_t storage_offset);
  Tensor asin() const;
  Tensor &amp; asin_();
  Tensor atan() const;
  Tensor &amp; atan_();
  Tensor baddbmm(const Tensor &amp; batch1, const Tensor &amp; batch2, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; baddbmm_(const Tensor &amp; batch1, const Tensor &amp; batch2, Scalar beta=1, Scalar alpha=1);
  Tensor bernoulli(Generator * generator=nullptr) const;
  Tensor &amp; bernoulli_(const Tensor &amp; p, Generator * generator=nullptr);
  Tensor &amp; bernoulli_(double p=0.5, Generator * generator=nullptr);
  Tensor bernoulli(double p, Generator * generator=nullptr) const;
  Tensor bincount(const Tensor &amp; weights={}, int64_t minlength=0) const;
  Tensor bmm(const Tensor &amp; mat2) const;
  Tensor ceil() const;
  Tensor &amp; ceil_();
  std::vector&lt;Tensor&gt; chunk(int64_t chunks, int64_t dim=0) const;
  Tensor clamp(c10::optional&lt;Scalar&gt; min=c10::nullopt, c10::optional&lt;Scalar&gt; max=c10::nullopt) const;
  Tensor &amp; clamp_(c10::optional&lt;Scalar&gt; min=c10::nullopt, c10::optional&lt;Scalar&gt; max=c10::nullopt);
  Tensor clamp_max(Scalar max) const;
  Tensor &amp; clamp_max_(Scalar max);
  Tensor clamp_min(Scalar min) const;
  Tensor &amp; clamp_min_(Scalar min);
  Tensor contiguous() const;
  Tensor cos() const;
  Tensor &amp; cos_();
  Tensor cosh() const;
  Tensor &amp; cosh_();
  Tensor cumsum(int64_t dim, ScalarType dtype) const;
  Tensor cumsum(int64_t dim) const;
  Tensor cumprod(int64_t dim, ScalarType dtype) const;
  Tensor cumprod(int64_t dim) const;
  Tensor det() const;
  Tensor diag_embed(int64_t offset=0, int64_t dim1=-2, int64_t dim2=-1) const;
  Tensor diagflat(int64_t offset=0) const;
  Tensor diagonal(int64_t offset=0, int64_t dim1=0, int64_t dim2=1) const;
  Tensor div(const Tensor &amp; other) const;
  Tensor &amp; div_(const Tensor &amp; other);
  Tensor div(Scalar other) const;
  Tensor &amp; div_(Scalar other);
  Tensor dot(const Tensor &amp; tensor) const;
  Tensor &amp; resize_(IntList size);
  Tensor erf() const;
  Tensor &amp; erf_();
  Tensor erfc() const;
  Tensor &amp; erfc_();
  Tensor exp() const;
  Tensor &amp; exp_();
  Tensor expm1() const;
  Tensor &amp; expm1_();
  Tensor expand(IntList size, bool implicit=false) const;
  Tensor expand_as(const Tensor &amp; other) const;
  Tensor flatten(int64_t start_dim=0, int64_t end_dim=-1) const;
  Tensor &amp; fill_(Scalar value);
  Tensor &amp; fill_(const Tensor &amp; value);
  Tensor floor() const;
  Tensor &amp; floor_();
  Tensor ger(const Tensor &amp; vec2) const;
  std::tuple&lt;Tensor,Tensor&gt; gesv(const Tensor &amp; A) const;
  Tensor fft(int64_t signal_ndim, bool normalized=false) const;
  Tensor ifft(int64_t signal_ndim, bool normalized=false) const;
  Tensor rfft(int64_t signal_ndim, bool normalized=false, bool onesided=true) const;
  Tensor irfft(int64_t signal_ndim, bool normalized=false, bool onesided=true, IntList signal_sizes={}) const;
  Tensor index(TensorList indices) const;
  Tensor &amp; index_copy_(int64_t dim, const Tensor &amp; index, const Tensor &amp; source);
  Tensor index_put(TensorList indices, const Tensor &amp; values, bool accumulate=false) const;
  Tensor &amp; index_put_(TensorList indices, const Tensor &amp; values, bool accumulate=false);
  Tensor inverse() const;
  Tensor isclose(const Tensor &amp; other, double rtol=1e-05, double atol=1e-08, bool equal_nan=false) const;
  bool is_distributed() const;
  bool is_floating_point() const;
  bool is_complex() const;
  bool is_nonzero() const;
  bool is_same_size(const Tensor &amp; other) const;
  bool is_signed() const;
  std::tuple&lt;Tensor,Tensor&gt; kthvalue(int64_t k, int64_t dim=-1, bool keepdim=false) const;
  Tensor log() const;
  Tensor &amp; log_();
  Tensor log10() const;
  Tensor &amp; log10_();
  Tensor log1p() const;
  Tensor &amp; log1p_();
  Tensor log2() const;
  Tensor &amp; log2_();
  Tensor logdet() const;
  Tensor log_softmax(int64_t dim, ScalarType dtype) const;
  Tensor log_softmax(int64_t dim) const;
  Tensor logsumexp(int64_t dim, bool keepdim=false) const;
  Tensor matmul(const Tensor &amp; other) const;
  Tensor matrix_power(int64_t n) const;
  std::tuple&lt;Tensor,Tensor&gt; max(int64_t dim, bool keepdim=false) const;
  Tensor max_values(int64_t dim, bool keepdim=false) const;
  Tensor mean(ScalarType dtype) const;
  Tensor mean() const;
  Tensor mean(IntList dim, bool keepdim, ScalarType dtype) const;
  Tensor mean(IntList dim, bool keepdim=false) const;
  Tensor mean(IntList dim, ScalarType dtype) const;
  std::tuple&lt;Tensor,Tensor&gt; median(int64_t dim, bool keepdim=false) const;
  std::tuple&lt;Tensor,Tensor&gt; min(int64_t dim, bool keepdim=false) const;
  Tensor min_values(int64_t dim, bool keepdim=false) const;
  Tensor mm(const Tensor &amp; mat2) const;
  std::tuple&lt;Tensor,Tensor&gt; mode(int64_t dim=-1, bool keepdim=false) const;
  Tensor mul(const Tensor &amp; other) const;
  Tensor &amp; mul_(const Tensor &amp; other);
  Tensor mul(Scalar other) const;
  Tensor &amp; mul_(Scalar other);
  Tensor mv(const Tensor &amp; vec) const;
  Tensor mvlgamma(int64_t p) const;
  Tensor &amp; mvlgamma_(int64_t p);
  Tensor narrow_copy(int64_t dim, int64_t start, int64_t length) const;
  Tensor narrow(int64_t dim, int64_t start, int64_t length) const;
  Tensor permute(IntList dims) const;
  Tensor pin_memory() const;
  Tensor pinverse(double rcond=1e-15) const;
  Tensor repeat(IntList repeats) const;
  Tensor reshape(IntList shape) const;
  Tensor reshape_as(const Tensor &amp; other) const;
  Tensor round() const;
  Tensor &amp; round_();
  Tensor relu() const;
  Tensor &amp; relu_();
  Tensor prelu(const Tensor &amp; weight) const;
  std::tuple&lt;Tensor,Tensor&gt; prelu_backward(const Tensor &amp; grad_output, const Tensor &amp; weight) const;
  Tensor hardshrink(Scalar lambd=0.5) const;
  Tensor hardshrink_backward(const Tensor &amp; grad_out, Scalar lambd) const;
  Tensor rsqrt() const;
  Tensor &amp; rsqrt_();
  Tensor select(int64_t dim, int64_t index) const;
  Tensor sigmoid() const;
  Tensor &amp; sigmoid_();
  Tensor sin() const;
  Tensor &amp; sin_();
  Tensor sinh() const;
  Tensor &amp; sinh_();
  Tensor detach() const;
  Tensor &amp; detach_();
  int64_t size(int64_t dim) const;
  Tensor slice(int64_t dim=0, int64_t start=0, int64_t end=9223372036854775807, int64_t step=1) const;
  std::tuple&lt;Tensor,Tensor&gt; slogdet() const;
  Tensor smm(const Tensor &amp; mat2) const;
  Tensor softmax(int64_t dim, ScalarType dtype) const;
  Tensor softmax(int64_t dim) const;
  std::vector&lt;Tensor&gt; split(int64_t split_size, int64_t dim=0) const;
  std::vector&lt;Tensor&gt; split_with_sizes(IntList split_sizes, int64_t dim=0) const;
  Tensor squeeze() const;
  Tensor squeeze(int64_t dim) const;
  Tensor &amp; squeeze_();
  Tensor &amp; squeeze_(int64_t dim);
  Tensor sspaddmm(const Tensor &amp; mat1, const Tensor &amp; mat2, Scalar beta=1, Scalar alpha=1) const;
  Tensor stft(int64_t n_fft, int64_t hop_length, int64_t win_length, const Tensor &amp; window={}, bool normalized=false, bool onesided=true) const;
  int64_t stride(int64_t dim) const;
  Tensor sum(ScalarType dtype) const;
  Tensor sum() const;
  Tensor sum(IntList dim, bool keepdim, ScalarType dtype) const;
  Tensor sum(IntList dim, bool keepdim=false) const;
  Tensor sum(IntList dim, ScalarType dtype) const;
  Tensor sqrt() const;
  Tensor &amp; sqrt_();
  Tensor std(bool unbiased=true) const;
  Tensor std(IntList dim, bool unbiased=true, bool keepdim=false) const;
  Tensor prod(ScalarType dtype) const;
  Tensor prod() const;
  Tensor prod(int64_t dim, bool keepdim, ScalarType dtype) const;
  Tensor prod(int64_t dim, bool keepdim=false) const;
  Tensor prod(int64_t dim, ScalarType dtype) const;
  Tensor t() const;
  Tensor &amp; t_();
  Tensor tan() const;
  Tensor &amp; tan_();
  Tensor tanh() const;
  Tensor &amp; tanh_();
  Tensor transpose(int64_t dim0, int64_t dim1) const;
  Tensor &amp; transpose_(int64_t dim0, int64_t dim1);
  Tensor flip(IntList dims) const;
  Tensor roll(IntList shifts, IntList dims={}) const;
  Tensor rot90(int64_t k=1, IntList dims={0,1}) const;
  Tensor trunc() const;
  Tensor &amp; trunc_();
  Tensor type_as(const Tensor &amp; other) const;
  Tensor unsqueeze(int64_t dim) const;
  Tensor &amp; unsqueeze_(int64_t dim);
  Tensor var(bool unbiased=true) const;
  Tensor var(int64_t dim, bool unbiased=true, bool keepdim=false) const;
  Tensor view_as(const Tensor &amp; other) const;
  Tensor where(const Tensor &amp; condition, const Tensor &amp; other) const;
  Tensor norm(Scalar p=2) const;
  Tensor norm(Scalar p, int64_t dim, bool keepdim=false) const;
  Tensor clone() const;
  Tensor &amp; resize_as_(const Tensor &amp; the_template);
  Tensor pow(Scalar exponent) const;
  Tensor &amp; zero_();
  Tensor sub(const Tensor &amp; other, Scalar alpha=1) const;
  Tensor &amp; sub_(const Tensor &amp; other, Scalar alpha=1);
  Tensor sub(Scalar other, Scalar alpha=1) const;
  Tensor &amp; sub_(Scalar other, Scalar alpha=1);
  Tensor addmm(const Tensor &amp; mat1, const Tensor &amp; mat2, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; addmm_(const Tensor &amp; mat1, const Tensor &amp; mat2, Scalar beta=1, Scalar alpha=1);
  Tensor &amp; sparse_resize_(IntList size, int64_t sparse_dim, int64_t dense_dim);
  Tensor &amp; sparse_resize_and_clear_(IntList size, int64_t sparse_dim, int64_t dense_dim);
  Tensor sparse_mask(SparseTensorRef mask) const;
  Tensor to_dense() const;
  int64_t sparse_dim() const;
  int64_t _dimI() const;
  int64_t dense_dim() const;
  int64_t _dimV() const;
  int64_t _nnz() const;
  Tensor coalesce() const;
  bool is_coalesced() const;
  Tensor _indices() const;
  Tensor _values() const;
  Tensor &amp; _coalesced_(bool coalesced);
  Tensor indices() const;
  Tensor values() const;
  int64_t numel() const;
  std::vector&lt;Tensor&gt; unbind(int64_t dim=0) const;
  Tensor to_sparse(int64_t sparse_dim) const;
  Tensor to_sparse() const;
  Tensor to(const TensorOptions &amp; options, bool non_blocking=false, bool copy=false) const;
  Tensor to(Device device, ScalarType dtype, bool non_blocking=false, bool copy=false) const;
  Tensor to(ScalarType dtype, bool non_blocking=false, bool copy=false) const;
  Tensor to(const Tensor &amp; other, bool non_blocking=false, bool copy=false) const;
  Scalar item() const;
  void* data_ptr() const;
  Tensor &amp; set_(Storage source);
  Tensor &amp; set_(Storage source, int64_t storage_offset, IntList size, IntList stride={});
  Tensor &amp; set_(const Tensor &amp; source);
  Tensor &amp; set_();
  bool is_set_to(const Tensor &amp; tensor) const;
  Tensor &amp; masked_fill_(const Tensor &amp; mask, Scalar value);
  Tensor &amp; masked_fill_(const Tensor &amp; mask, const Tensor &amp; value);
  Tensor &amp; masked_scatter_(const Tensor &amp; mask, const Tensor &amp; source);
  Tensor view(IntList size) const;
  Tensor &amp; put_(const Tensor &amp; index, const Tensor &amp; source, bool accumulate=false);
  Tensor &amp; index_add_(int64_t dim, const Tensor &amp; index, const Tensor &amp; source);
  Tensor &amp; index_fill_(int64_t dim, const Tensor &amp; index, Scalar value);
  Tensor &amp; index_fill_(int64_t dim, const Tensor &amp; index, const Tensor &amp; value);
  Tensor &amp; scatter_(int64_t dim, const Tensor &amp; index, const Tensor &amp; src);
  Tensor &amp; scatter_(int64_t dim, const Tensor &amp; index, Scalar value);
  Tensor &amp; scatter_add_(int64_t dim, const Tensor &amp; index, const Tensor &amp; src);
  Tensor &amp; lt_(Scalar other);
  Tensor &amp; lt_(const Tensor &amp; other);
  Tensor &amp; gt_(Scalar other);
  Tensor &amp; gt_(const Tensor &amp; other);
  Tensor &amp; le_(Scalar other);
  Tensor &amp; le_(const Tensor &amp; other);
  Tensor &amp; ge_(Scalar other);
  Tensor &amp; ge_(const Tensor &amp; other);
  Tensor &amp; eq_(Scalar other);
  Tensor &amp; eq_(const Tensor &amp; other);
  Tensor &amp; ne_(Scalar other);
  Tensor &amp; ne_(const Tensor &amp; other);
  Tensor __and__(Scalar other) const;
  Tensor __and__(const Tensor &amp; other) const;
  Tensor &amp; __iand__(Scalar other);
  Tensor &amp; __iand__(const Tensor &amp; other);
  Tensor __or__(Scalar other) const;
  Tensor __or__(const Tensor &amp; other) const;
  Tensor &amp; __ior__(Scalar other);
  Tensor &amp; __ior__(const Tensor &amp; other);
  Tensor __xor__(Scalar other) const;
  Tensor __xor__(const Tensor &amp; other) const;
  Tensor &amp; __ixor__(Scalar other);
  Tensor &amp; __ixor__(const Tensor &amp; other);
  Tensor __lshift__(Scalar other) const;
  Tensor __lshift__(const Tensor &amp; other) const;
  Tensor &amp; __ilshift__(Scalar other);
  Tensor &amp; __ilshift__(const Tensor &amp; other);
  Tensor __rshift__(Scalar other) const;
  Tensor __rshift__(const Tensor &amp; other) const;
  Tensor &amp; __irshift__(Scalar other);
  Tensor &amp; __irshift__(const Tensor &amp; other);
  Tensor &amp; lgamma_();
  Tensor &amp; atan2_(const Tensor &amp; other);
  Tensor &amp; tril_(int64_t diagonal=0);
  Tensor &amp; triu_(int64_t diagonal=0);
  Tensor &amp; digamma_();
  Tensor &amp; polygamma_(int64_t n);
  Tensor &amp; erfinv_();
  Tensor &amp; frac_();
  Tensor &amp; renorm_(Scalar p, int64_t dim, Scalar maxnorm);
  Tensor &amp; reciprocal_();
  Tensor &amp; neg_();
  Tensor &amp; pow_(Scalar exponent);
  Tensor &amp; pow_(const Tensor &amp; exponent);
  Tensor &amp; lerp_(const Tensor &amp; end, Scalar weight);
  Tensor &amp; sign_();
  Tensor &amp; fmod_(Scalar other);
  Tensor &amp; fmod_(const Tensor &amp; other);
  Tensor &amp; remainder_(Scalar other);
  Tensor &amp; remainder_(const Tensor &amp; other);
  Tensor &amp; addbmm_(const Tensor &amp; batch1, const Tensor &amp; batch2, Scalar beta=1, Scalar alpha=1);
  Tensor addbmm(const Tensor &amp; batch1, const Tensor &amp; batch2, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; addcmul_(const Tensor &amp; tensor1, const Tensor &amp; tensor2, Scalar value=1);
  Tensor &amp; addcdiv_(const Tensor &amp; tensor1, const Tensor &amp; tensor2, Scalar value=1);
  Tensor &amp; random_(int64_t from, int64_t to, Generator * generator=nullptr);
  Tensor &amp; random_(int64_t to, Generator * generator=nullptr);
  Tensor &amp; random_(Generator * generator=nullptr);
  Tensor &amp; uniform_(double from=0, double to=1, Generator * generator=nullptr);
  Tensor &amp; normal_(double mean=0, double std=1, Generator * generator=nullptr);
  Tensor &amp; cauchy_(double median=0, double sigma=1, Generator * generator=nullptr);
  Tensor &amp; log_normal_(double mean=1, double std=2, Generator * generator=nullptr);
  Tensor &amp; exponential_(double lambd=1, Generator * generator=nullptr);
  Tensor &amp; geometric_(double p, Generator * generator=nullptr);
  Tensor diag(int64_t diagonal=0) const;
  Tensor cross(const Tensor &amp; other, int64_t dim=-1) const;
  Tensor triu(int64_t diagonal=0) const;
  Tensor tril(int64_t diagonal=0) const;
  Tensor trace() const;
  Tensor ne(Scalar other) const;
  Tensor ne(const Tensor &amp; other) const;
  Tensor eq(Scalar other) const;
  Tensor eq(const Tensor &amp; other) const;
  Tensor ge(Scalar other) const;
  Tensor ge(const Tensor &amp; other) const;
  Tensor le(Scalar other) const;
  Tensor le(const Tensor &amp; other) const;
  Tensor gt(Scalar other) const;
  Tensor gt(const Tensor &amp; other) const;
  Tensor lt(Scalar other) const;
  Tensor lt(const Tensor &amp; other) const;
  Tensor take(const Tensor &amp; index) const;
  Tensor index_select(int64_t dim, const Tensor &amp; index) const;
  Tensor masked_select(const Tensor &amp; mask) const;
  Tensor nonzero() const;
  Tensor gather(int64_t dim, const Tensor &amp; index) const;
  Tensor addcmul(const Tensor &amp; tensor1, const Tensor &amp; tensor2, Scalar value=1) const;
  Tensor addcdiv(const Tensor &amp; tensor1, const Tensor &amp; tensor2, Scalar value=1) const;
  std::tuple&lt;Tensor,Tensor&gt; gels(const Tensor &amp; A) const;
  std::tuple&lt;Tensor,Tensor&gt; trtrs(const Tensor &amp; A, bool upper=true, bool transpose=false, bool unitriangular=false) const;
  std::tuple&lt;Tensor,Tensor&gt; symeig(bool eigenvectors=false, bool upper=true) const;
  std::tuple&lt;Tensor,Tensor&gt; eig(bool eigenvectors=false) const;
  std::tuple&lt;Tensor,Tensor,Tensor&gt; svd(bool some=true, bool compute_uv=true) const;
  Tensor cholesky(bool upper=false) const;
  Tensor potrs(const Tensor &amp; input2, bool upper=true) const;
  Tensor potri(bool upper=true) const;
  std::tuple&lt;Tensor,Tensor&gt; pstrf(bool upper=true, Scalar tol=-1) const;
  std::tuple&lt;Tensor,Tensor&gt; qr() const;
  std::tuple&lt;Tensor,Tensor&gt; geqrf() const;
  Tensor orgqr(const Tensor &amp; input2) const;
  Tensor ormqr(const Tensor &amp; input2, const Tensor &amp; input3, bool left=true, bool transpose=false) const;
  std::tuple&lt;Tensor,Tensor&gt; btrifact(bool pivot=true) const;
  std::tuple&lt;Tensor,Tensor,Tensor&gt; btrifact_with_info(bool pivot=true) const;
  Tensor btrisolve(const Tensor &amp; LU_data, const Tensor &amp; LU_pivots) const;
  Tensor multinomial(int64_t num_samples, bool replacement=false, Generator * generator=nullptr) const;
  Tensor lgamma() const;
  Tensor digamma() const;
  Tensor polygamma(int64_t n) const;
  Tensor erfinv() const;
  Tensor frac() const;
  Tensor dist(const Tensor &amp; other, Scalar p=2) const;
  Tensor reciprocal() const;
  Tensor neg() const;
  Tensor atan2(const Tensor &amp; other) const;
  Tensor lerp(const Tensor &amp; end, Scalar weight) const;
  Tensor histc(int64_t bins=100, Scalar min=0, Scalar max=0) const;
  Tensor sign() const;
  Tensor fmod(Scalar other) const;
  Tensor fmod(const Tensor &amp; other) const;
  Tensor remainder(Scalar other) const;
  Tensor remainder(const Tensor &amp; other) const;
  Tensor min(const Tensor &amp; other) const;
  Tensor min() const;
  Tensor max(const Tensor &amp; other) const;
  Tensor max() const;
  Tensor median() const;
  std::tuple&lt;Tensor,Tensor&gt; sort(int64_t dim=-1, bool descending=false) const;
  std::tuple&lt;Tensor,Tensor&gt; topk(int64_t k, int64_t dim=-1, bool largest=true, bool sorted=true) const;
  Tensor all() const;
  Tensor any() const;
  Tensor renorm(Scalar p, int64_t dim, Scalar maxnorm) const;
  Tensor unfold(int64_t dimension, int64_t size, int64_t step) const;
  bool equal(const Tensor &amp; other) const;
  Tensor pow(const Tensor &amp; exponent) const;
  Tensor alias() const;

  // We changed .dtype() to return a TypeMeta in #12766. Ideally, we want the
  // at::kDouble and its friends to be TypeMeta&#39;s, but that hasn&#39;t happened yet.
  // Before that change, we make this method to maintain BC for C++ usage like
  // `x.to(y.dtype)`.
  // TODO: remove following two after at::kDouble and its friends are TypeMeta&#39;s.
  inline Tensor to(caffe2::TypeMeta type_meta, bool non_blocking=false, bool copy=false) const {
    return this-&gt;to(/*scalar_type=*/typeMetaToScalarType(type_meta), non_blocking, copy);
  }
  inline Tensor to(Device device, caffe2::TypeMeta type_meta, bool non_blocking=false, bool copy=false) const {
    return this-&gt;to(device, /*scalar_type=*/typeMetaToScalarType(type_meta), non_blocking, copy);
  }

  template &lt;typename F, typename... Args&gt;
  auto m(F func, Args&amp;&amp;... params) const -&gt; decltype(func(*this, std::forward&lt;Args&gt;(params)...)) {
    return func(*this, std::forward&lt;Args&gt;(params)...);
  }

  friend struct WeakTensor;

protected:
  c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt; impl_;
};

struct CAFFE2_API WeakTensor {
  WeakTensor(const Tensor&amp; t) : weak_impl_(t.impl_) {}

  // XXX: this can return undefined tensors
  // Ideally it would be c10::optional&lt;Tensor&gt;, but MSVC is too cool for that
  Tensor lock() const {
    return Tensor(weak_impl_.lock());
  }

  bool is_same(const WeakTensor&amp; other) const noexcept {
    return weak_impl_ == other.weak_impl_;
  }

  size_t use_count() const noexcept {
    return weak_impl_.use_count();
  }
  size_t weak_use_count() const noexcept {
    return weak_impl_.weak_use_count();
  }

  TensorImpl* unsafeGetTensorImpl() const {
    return weak_impl_._unsafe_get_target();
  }

private:
  c10::weak_intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt; weak_impl_;
};

namespace detail {
// Helper creator for Tensor clas which doesn&#39;t requires the users to pass
// in an intrusive_ptr instead it just converts the argument passed to
// requested intrusive_ptr type.
template &lt;typename T, typename... Args&gt;
Tensor make_tensor(Args&amp;&amp;... args) {
  return Tensor(c10::make_intrusive&lt;T&gt;(std::forward&lt;Args&gt;(args)...));
}
} // namespace detail

} // namespace at

#include &lt;ATen/core/TensorMethods.h&gt;
</pre></div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="file_torch_csrc_api_include_torch_data_datasets_tensor.h.html" class="btn btn-neutral float-right" title="File tensor.h" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="file_aten_src_ATen_core_Tensor.h.html" class="btn btn-neutral" title="File Tensor.h" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Program Listing for File Tensor.h</a></li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../_static/jquery.js"></script>
         <script type="text/javascript" src="../_static/underscore.js"></script>
         <script type="text/javascript" src="../_static/doctools.js"></script>
         <script type="text/javascript" src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
         <script type="text/javascript" src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>