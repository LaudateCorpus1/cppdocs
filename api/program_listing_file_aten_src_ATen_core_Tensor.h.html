


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Program Listing for File Tensor.h &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/api/program_listing_file_aten_src_ATen_core_Tensor.h.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../installing.html">Installing C++ Distributions of PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../frontend.html">The C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing to PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="library_root.html">Library API</a></li>
</ul>
<p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notes/faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_basics.html">Tensor Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_creation.html">Tensor Creation API</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Program Listing for File Tensor.h</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            
              <!-- User defined GitHub URL -->
              <a href="https://github.com/pytorch/pytorch" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="program-listing-for-file-tensor-h">
<span id="program-listing-file-aten-src-aten-core-tensor-h"></span><h1>Program Listing for File Tensor.h<a class="headerlink" href="#program-listing-for-file-tensor-h" title="Permalink to this headline">¶</a></h1>
<p>↰ <a class="reference internal" href="file_aten_src_ATen_core_Tensor.h.html#file-aten-src-aten-core-tensor-h"><span class="std std-ref">Return to documentation for file</span></a> (<code class="docutils literal notranslate"><span class="pre">aten/src/ATen/core/Tensor.h</span></code>)</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>#pragma once

#include &lt;c10/core/Device.h&gt;
#include &lt;c10/core/Layout.h&gt;
#include &lt;c10/core/MemoryFormat.h&gt;
#include &lt;c10/core/QScheme.h&gt;
#include &lt;c10/core/Scalar.h&gt;
#include &lt;c10/core/ScalarType.h&gt;
#include &lt;c10/core/Storage.h&gt;
#include &lt;ATen/core/TensorAccessor.h&gt;
#include &lt;ATen/core/TensorOptions.h&gt;
#include &lt;c10/core/TensorImpl.h&gt;
#include &lt;c10/core/UndefinedTensorImpl.h&gt;
#include &lt;c10/util/Exception.h&gt;
#include &lt;c10/util/Optional.h&gt;
#include &lt;c10/util/intrusive_ptr.h&gt;
#include &lt;ATen/core/LegacyTypeDispatch.h&gt;
#include &lt;ATen/core/DeprecatedTypePropertiesRegistry.h&gt;
#ifdef BUILD_NAMEDTENSOR
#include &lt;ATen/NamedTensor.h&gt;
#endif

namespace caffe2 {
class Tensor;
}
namespace at {
struct Generator;
struct Type;
class DeprecatedTypeProperties;
class Tensor;
} // namespace at

namespace at {

class Tensor;
using TensorList = ArrayRef&lt;Tensor&gt;;

struct Quantizer;
// This is temporary typedef to enable Quantizer in aten native function API
// we&#39;ll remove them when we are actually exposing Quantizer class
// to frontend
using ConstQuantizerPtr = const c10::intrusive_ptr&lt;Quantizer&gt;&amp;;

// Tensor is a &quot;generic&quot; object holding a pointer to the underlying TensorImpl object, which
// has an embedded reference count. In this way, Tensor is similar to boost::intrusive_ptr.
//
// For example:
//
// void func(Tensor a) {
//   Tensor b = a;
//   ...
// }
//
// In this example, when we say Tensor b = a, we are creating a new object that points to the
// same underlying TensorImpl, and bumps its reference count. When b goes out of scope, the
// destructor decrements the reference count by calling release() on the TensorImpl it points to.
// The existing constructors, operator overloads, etc. take care to implement the correct semantics.
//
// Note that Tensor can also be NULL, i.e. it is not associated with any underlying TensorImpl, and
// special care must be taken to handle this.
class CAFFE2_API Tensor {
 public:
  Tensor(){};
  // This constructor should not be used by end users and is an implementation
  // detail invoked by autogenerated code.
  explicit Tensor(
      c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt; tensor_impl)
      : impl_(std::move(tensor_impl)) {
    if (impl_.get() == nullptr) {
      throw std::runtime_error(&quot;TensorImpl with nullptr is not supported&quot;);
    }
  }
  Tensor(const Tensor&amp;) = default;
  Tensor(Tensor&amp;&amp;) = default;


 public:
  // Creates a new wrapper from TensorImpl. Intentionally a free method because
  // it should be used with care. Checks necessary invariants
  static Tensor wrap_tensor_impl(
      c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt; tensor_impl) {
    Tensor r(std::move(tensor_impl));
    r.enforce_invariants();
    return r;
  }

  int64_t dim() const {
    return impl_-&gt;dim();
  }
  int64_t storage_offset() const {
    return impl_-&gt;storage_offset();
  }

  TensorImpl * unsafeGetTensorImpl() const {
    return impl_.get();
  }
  TensorImpl * unsafeReleaseTensorImpl() {
    return impl_.release();
  }
  const c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt;&amp; getIntrusivePtr() const {
    return impl_;
  }

  bool defined() const {
    return impl_;
  }

  void reset() {
    impl_.reset();
  }

  // The following overloads are very intruiging.  Consider the following
  // program:
  //
  //    x[1] = 3;
  //
  // We would expect that the first entry of x is written to 3.  But how can we
  // actually achieve this?  x[1] evaluates to a tensor...
  //
  // The answer is, using a ref-qualifier.  x[1] is an rvalue, which cannot be
  // (profitably) assigned to in the traditional sense, so we overload
  // assignment to mean, &quot;Actually, copy 3 into the tensor data.&quot;  This is done
  // with an rvalue-reference ref-qualified overload (the methods with &amp;&amp; at the
  // end of their type.)
  //
  // There&#39;s one more fly in the ointment: We also want
  //
  //    Tensor x = y;
  //
  // to work, and we want it NOT to copy.  So we need a traditional operator=
  // overload.  But we MUST specify a mutable lvalue ref-qualifier, to
  // disambiguate the traditional overload from the rvalue-reference
  // ref-qualified overload.  Otherwise, it will be ambiguous, because
  // a non ref-qualified method is eligible for all situations.

  // Unfortunately, we have to write these constructors out manually
  // to work around an MSVC bug:
  //    error C2580: &#39;at::Tensor &amp;at::Tensor::operator =(const at::Tensor &amp;) &amp;&#39;:
  //    multiple versions of a defaulted special member functions are not allowed
  // Tensor&amp; operator=(const Tensor&amp;) &amp; = default;
  // Tensor&amp; operator=(Tensor&amp;&amp;) &amp; = default;
  Tensor&amp; operator=(const Tensor&amp; x) &amp; {
    impl_ = x.impl_;
    return *this;
  }
  Tensor&amp; operator=(Tensor&amp;&amp; x) &amp; {
    impl_ = std::move(x.impl_);
    return *this;
  }

  Tensor&amp; operator=(Scalar v) &amp;&amp;;
  Tensor&amp; operator=(const Tensor&amp;) &amp;&amp;;
  Tensor&amp; operator=(Tensor&amp;&amp;) &amp;&amp;;

  bool is_same(const Tensor&amp; other) const noexcept {
    return impl_ == other.impl_;
  }
  size_t use_count() const noexcept {
    return impl_.use_count();
  }
  size_t weak_use_count() const noexcept {
    return impl_.weak_use_count();
  }

  std::string toString() const;

  IntArrayRef sizes() const {
    return impl_-&gt;sizes();
  }
  IntArrayRef strides() const {
    return impl_-&gt;strides();
  }
#ifdef BUILD_NAMEDTENSOR
  optional&lt;DimnameList&gt; names() const {
    return impl::internal_get_names(unsafeGetTensorImpl());
  }
#endif
  int64_t ndimension() const {
    return dim();
  }
  bool is_contiguous(at::MemoryFormat memory_format=at::MemoryFormat::Contiguous) const {
    return impl_-&gt;is_contiguous(memory_format);
  }

  at::MemoryFormat suggest_memory_format() const {
    if (impl_-&gt;is_strides_like_channels_last()) {
      return at::MemoryFormat::ChannelsLast;
    }
    return at::MemoryFormat::Contiguous;
  }

  // Total bytes consumed by the &quot;view&quot; of elements of the array.  Does not
  // include size of metadata.  The number reported here does not necessarily
  // correspond to the true physical memory consumed by a tensor; instead,
  // it reports the memory the tensor would take *if* it were contiguous.
  // Defined to be numel() * itemsize()
  size_t nbytes() const {
    return impl_-&gt;numel() * impl_-&gt;itemsize();
  }

  // Length of one array element in bytes.  This is the traditional
  // Numpy naming.
  size_t itemsize() const {
    return impl_-&gt;itemsize();
  }

  // Same as itemsize().  This is the PyTorch naming.
  size_t element_size() const {
    return impl_-&gt;itemsize();
  }

  DeprecatedTypeProperties &amp; type() const {
    return globalDeprecatedTypePropertiesRegistry().getDeprecatedTypeProperties(
        tensorTypeIdToBackend(type_id()),
        scalar_type(),
        is_variable());
  }
  TensorTypeId type_id() const {
    return impl_-&gt;type_id();
  }
  ScalarType scalar_type() const {
    return typeMetaToScalarType(impl_-&gt;dtype());
  }
  bool has_storage() const {
    return defined() &amp;&amp; impl_-&gt;has_storage();
  }
  const Storage&amp; storage() const {
    return impl_-&gt;storage();
  }
  bool is_alias_of(const at::Tensor&amp; other) const{
    return impl_-&gt;storage().is_alias_of(other.storage());
  }
  Tensor toType(const DeprecatedTypeProperties &amp; t, bool non_blocking=false) const;
  Tensor toType(ScalarType t) const;
  Tensor toBackend(Backend b) const;

  bool is_variable() const noexcept;

  Layout layout() const noexcept;

  caffe2::TypeMeta dtype() const noexcept;

  Device device() const;

  int64_t get_device() const;

  bool is_cuda() const;

  bool is_hip() const;

  bool is_sparse() const;

  bool is_mkldnn() const;

  bool is_quantized() const;

#ifdef BUILD_NAMEDTENSOR
  bool has_names() const;

  const NamedTensorMeta* get_named_tensor_meta() const;
  NamedTensorMeta* get_named_tensor_meta();
#endif

  TensorOptions options() const;

  void* data_ptr() const {
    return this-&gt;unsafeGetTensorImpl()-&gt;data();
  }

  template&lt;typename T&gt;
  T * data() const;

  template &lt;typename T&gt;
  T item() const;

  // Purposely not defined here to avoid inlining
  void print() const;

  // Return a `TensorAccessor` for CPU `Tensor`s. You have to specify scalar type and
  // dimension.
  template&lt;typename T, size_t N&gt;
  TensorAccessor&lt;T,N&gt; accessor() const&amp; {
    static_assert(N &gt; 0, &quot;accessor is used for indexing tensor, for scalars use *data&lt;T&gt;()&quot;);
    TORCH_CHECK(dim() == N, &quot;expected &quot;, N, &quot; dims but tensor has &quot;, dim());
    return TensorAccessor&lt;T,N&gt;(data&lt;T&gt;(),sizes().data(),strides().data());
  }
  template&lt;typename T, size_t N&gt;
  TensorAccessor&lt;T,N&gt; accessor() &amp;&amp; = delete;

  // Return a `PackedTensorAccessor` for CUDA `Tensor`s. You have to specify scalar type and
  // dimension. You can optionally specify RestrictPtrTraits as a template parameter to
  // cast the data pointer to a __restrict__ pointer.
  // In order to use this, your CUDA kernel has to take a corresponding PackedTensorAccessor
  // as an argument.
  template&lt;typename T, size_t N, template &lt;typename U&gt; class PtrTraits = DefaultPtrTraits, typename index_t = int64_t&gt;
  PackedTensorAccessor&lt;T,N,PtrTraits,index_t&gt; packed_accessor() const&amp; {
    static_assert(N &gt; 0, &quot;accessor is used for indexing tensor, for scalars use *data&lt;T&gt;()&quot;);
    TORCH_CHECK(dim() == N, &quot;expected &quot;, N, &quot; dims but tensor has &quot;, dim());
    return PackedTensorAccessor&lt;T,N,PtrTraits,index_t&gt;(static_cast&lt;typename PtrTraits&lt;T&gt;::PtrType&gt;(data&lt;T&gt;()),sizes().data(),strides().data());
  }
  template&lt;typename T, size_t N,  template &lt;typename U&gt; class PtrTraits = DefaultPtrTraits, typename index_t = int64_t&gt;
  PackedTensorAccessor&lt;T,N&gt; packed_accessor() &amp;&amp; = delete;

  Tensor operator-() const;
  Tensor&amp; operator+=(const Tensor &amp; other);
  Tensor&amp; operator+=(Scalar other);
  Tensor&amp; operator-=(const Tensor &amp; other);
  Tensor&amp; operator-=(Scalar other);
  Tensor&amp; operator*=(const Tensor &amp; other);
  Tensor&amp; operator*=(Scalar other);
  Tensor&amp; operator/=(const Tensor &amp; other);
  Tensor&amp; operator/=(Scalar other);
  Tensor operator[](Scalar index) const;
  Tensor operator[](Tensor index) const;
  Tensor operator[](int64_t index) const;

  Tensor cpu() const;
  Tensor cuda() const;
  Tensor hip() const;

  // ~~~~~ Autograd API ~~~~~

  Tensor&amp; set_requires_grad(bool requires_grad) {
    impl_-&gt;set_requires_grad(requires_grad);
    return *this;
  }
  bool requires_grad() const {
    return impl_-&gt;requires_grad();
  }

  Tensor&amp; grad() {
    return impl_-&gt;grad();
  }
  const Tensor&amp; grad() const {
    return impl_-&gt;grad();
  }

  // STOP.  Thinking of adding a method here, which only makes use
  // of other ATen methods?  Define it in native_functions.yaml.

  //example
  //Tensor * add(Tensor &amp; b);
  void backward(const Tensor &amp; gradient={}, bool keep_graph=false, bool create_graph=false) const;
  void set_data(const Tensor &amp; new_data) const;
  #ifdef BUILD_NAMEDTENSOR
  Tensor &amp; set_names_(c10::optional&lt;DimnameList&gt; names);
  #endif
  Tensor abs() const;
  Tensor &amp; abs_();
  Tensor acos() const;
  Tensor &amp; acos_();
  Tensor add(const Tensor &amp; other, Scalar alpha=1) const;
  Tensor &amp; add_(const Tensor &amp; other, Scalar alpha=1);
  Tensor add(Scalar other, Scalar alpha=1) const;
  Tensor &amp; add_(Scalar other, Scalar alpha=1);
  Tensor addmv(const Tensor &amp; mat, const Tensor &amp; vec, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; addmv_(const Tensor &amp; mat, const Tensor &amp; vec, Scalar beta=1, Scalar alpha=1);
  Tensor addr(const Tensor &amp; vec1, const Tensor &amp; vec2, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; addr_(const Tensor &amp; vec1, const Tensor &amp; vec2, Scalar beta=1, Scalar alpha=1);
  Tensor all(int64_t dim, bool keepdim=false) const;
  bool allclose(const Tensor &amp; other, double rtol=1e-05, double atol=1e-08, bool equal_nan=false) const;
  Tensor any(int64_t dim, bool keepdim=false) const;
  Tensor argmax(c10::optional&lt;int64_t&gt; dim=c10::nullopt, bool keepdim=false) const;
  Tensor argmin(c10::optional&lt;int64_t&gt; dim=c10::nullopt, bool keepdim=false) const;
  Tensor as_strided(IntArrayRef size, IntArrayRef stride, c10::optional&lt;int64_t&gt; storage_offset=c10::nullopt) const;
  Tensor &amp; as_strided_(IntArrayRef size, IntArrayRef stride, c10::optional&lt;int64_t&gt; storage_offset=c10::nullopt);
  Tensor asin() const;
  Tensor &amp; asin_();
  Tensor atan() const;
  Tensor &amp; atan_();
  Tensor baddbmm(const Tensor &amp; batch1, const Tensor &amp; batch2, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; baddbmm_(const Tensor &amp; batch1, const Tensor &amp; batch2, Scalar beta=1, Scalar alpha=1);
  Tensor bernoulli(Generator * generator=nullptr) const;
  Tensor &amp; bernoulli_(const Tensor &amp; p, Generator * generator=nullptr);
  Tensor &amp; bernoulli_(double p=0.5, Generator * generator=nullptr);
  Tensor bernoulli(double p, Generator * generator=nullptr) const;
  Tensor bincount(const Tensor &amp; weights={}, int64_t minlength=0) const;
  Tensor bitwise_not() const;
  Tensor &amp; bitwise_not_();
  Tensor bmm(const Tensor &amp; mat2) const;
  Tensor ceil() const;
  Tensor &amp; ceil_();
  std::vector&lt;Tensor&gt; chunk(int64_t chunks, int64_t dim=0) const;
  Tensor clamp(c10::optional&lt;Scalar&gt; min=c10::nullopt, c10::optional&lt;Scalar&gt; max=c10::nullopt) const;
  Tensor &amp; clamp_(c10::optional&lt;Scalar&gt; min=c10::nullopt, c10::optional&lt;Scalar&gt; max=c10::nullopt);
  Tensor clamp_max(Scalar max) const;
  Tensor &amp; clamp_max_(Scalar max);
  Tensor clamp_min(Scalar min) const;
  Tensor &amp; clamp_min_(Scalar min);
  Tensor contiguous(MemoryFormat memory_format=MemoryFormat::Contiguous) const;
  Tensor &amp; copy_(const Tensor &amp; src, bool non_blocking=false);
  Tensor cos() const;
  Tensor &amp; cos_();
  Tensor cosh() const;
  Tensor &amp; cosh_();
  Tensor cumsum(int64_t dim, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor cumprod(int64_t dim, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor det() const;
  Tensor diag_embed(int64_t offset=0, int64_t dim1=-2, int64_t dim2=-1) const;
  Tensor diagflat(int64_t offset=0) const;
  Tensor diagonal(int64_t offset=0, int64_t dim1=0, int64_t dim2=1) const;
  Tensor &amp; fill_diagonal_(Scalar fill_value, bool wrap=false);
  Tensor div(const Tensor &amp; other) const;
  Tensor &amp; div_(const Tensor &amp; other);
  Tensor div(Scalar other) const;
  Tensor &amp; div_(Scalar other);
  Tensor dot(const Tensor &amp; tensor) const;
  Tensor &amp; resize_(IntArrayRef size);
  Tensor erf() const;
  Tensor &amp; erf_();
  Tensor erfc() const;
  Tensor &amp; erfc_();
  Tensor exp() const;
  Tensor &amp; exp_();
  Tensor expm1() const;
  Tensor &amp; expm1_();
  Tensor expand(IntArrayRef size, bool implicit=false) const;
  Tensor expand_as(const Tensor &amp; other) const;
  Tensor flatten(int64_t start_dim=0, int64_t end_dim=-1) const;
  Tensor &amp; fill_(Scalar value);
  Tensor &amp; fill_(const Tensor &amp; value);
  Tensor floor() const;
  Tensor &amp; floor_();
  Tensor frac() const;
  Tensor &amp; frac_();
  Tensor ger(const Tensor &amp; vec2) const;
  Tensor fft(int64_t signal_ndim, bool normalized=false) const;
  Tensor ifft(int64_t signal_ndim, bool normalized=false) const;
  Tensor rfft(int64_t signal_ndim, bool normalized=false, bool onesided=true) const;
  Tensor irfft(int64_t signal_ndim, bool normalized=false, bool onesided=true, IntArrayRef signal_sizes={}) const;
  Tensor index(TensorList indices) const;
  Tensor &amp; index_copy_(int64_t dim, const Tensor &amp; index, const Tensor &amp; source);
  Tensor index_copy(int64_t dim, const Tensor &amp; index, const Tensor &amp; source) const;
  Tensor &amp; index_put_(TensorList indices, const Tensor &amp; values, bool accumulate=false);
  Tensor index_put(TensorList indices, const Tensor &amp; values, bool accumulate=false) const;
  Tensor inverse() const;
  Tensor isclose(const Tensor &amp; other, double rtol=1e-05, double atol=1e-08, bool equal_nan=false) const;
  bool is_distributed() const;
  bool is_floating_point() const;
  bool is_complex() const;
  bool is_nonzero() const;
  bool is_same_size(const Tensor &amp; other) const;
  bool is_signed() const;
  std::tuple&lt;Tensor,Tensor&gt; kthvalue(int64_t k, int64_t dim=-1, bool keepdim=false) const;
  Tensor log() const;
  Tensor &amp; log_();
  Tensor log10() const;
  Tensor &amp; log10_();
  Tensor log1p() const;
  Tensor &amp; log1p_();
  Tensor log2() const;
  Tensor &amp; log2_();
  Tensor logdet() const;
  Tensor log_softmax(int64_t dim, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor logsumexp(IntArrayRef dim, bool keepdim=false) const;
  Tensor matmul(const Tensor &amp; other) const;
  Tensor matrix_power(int64_t n) const;
  std::tuple&lt;Tensor,Tensor&gt; max(int64_t dim, bool keepdim=false) const;
  Tensor max_values(IntArrayRef dim, bool keepdim=false) const;
  Tensor mean(c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor mean(IntArrayRef dim, bool keepdim=false, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  std::tuple&lt;Tensor,Tensor&gt; median(int64_t dim, bool keepdim=false) const;
  std::tuple&lt;Tensor,Tensor&gt; min(int64_t dim, bool keepdim=false) const;
  Tensor min_values(IntArrayRef dim, bool keepdim=false) const;
  Tensor mm(const Tensor &amp; mat2) const;
  std::tuple&lt;Tensor,Tensor&gt; mode(int64_t dim=-1, bool keepdim=false) const;
  Tensor mul(const Tensor &amp; other) const;
  Tensor &amp; mul_(const Tensor &amp; other);
  Tensor mul(Scalar other) const;
  Tensor &amp; mul_(Scalar other);
  Tensor mv(const Tensor &amp; vec) const;
  Tensor mvlgamma(int64_t p) const;
  Tensor &amp; mvlgamma_(int64_t p);
  Tensor narrow_copy(int64_t dim, int64_t start, int64_t length) const;
  Tensor narrow(int64_t dim, int64_t start, int64_t length) const;
  Tensor permute(IntArrayRef dims) const;
  Tensor numpy_T() const;
  bool is_pinned() const;
  Tensor pin_memory() const;
  Tensor pinverse(double rcond=1e-15) const;
  Tensor reciprocal() const;
  Tensor &amp; reciprocal_();
  Tensor neg() const;
  Tensor &amp; neg_();
  Tensor repeat(IntArrayRef repeats) const;
  Tensor repeat_interleave(const Tensor &amp; repeats, c10::optional&lt;int64_t&gt; dim=c10::nullopt) const;
  Tensor repeat_interleave(int64_t repeats, c10::optional&lt;int64_t&gt; dim=c10::nullopt) const;
  Tensor reshape(IntArrayRef shape) const;
  Tensor reshape_as(const Tensor &amp; other) const;
  Tensor round() const;
  Tensor &amp; round_();
  Tensor relu() const;
  Tensor &amp; relu_();
  Tensor prelu(const Tensor &amp; weight) const;
  std::tuple&lt;Tensor,Tensor&gt; prelu_backward(const Tensor &amp; grad_output, const Tensor &amp; weight) const;
  Tensor hardshrink(Scalar lambd=0.5) const;
  Tensor hardshrink_backward(const Tensor &amp; grad_out, Scalar lambd) const;
  Tensor rsqrt() const;
  Tensor &amp; rsqrt_();
  #ifdef BUILD_NAMEDTENSOR
  Tensor select(Dimname dim, int64_t index) const;
  #endif
  Tensor select(int64_t dim, int64_t index) const;
  Tensor sigmoid() const;
  Tensor &amp; sigmoid_();
  Tensor sin() const;
  Tensor &amp; sin_();
  Tensor sinh() const;
  Tensor &amp; sinh_();
  Tensor detach() const;
  Tensor &amp; detach_();
  int64_t size(int64_t dim) const;
  #ifdef BUILD_NAMEDTENSOR
  int64_t size(Dimname dim) const;
  #endif
  Tensor slice(int64_t dim=0, int64_t start=0, int64_t end=9223372036854775807, int64_t step=1) const;
  std::tuple&lt;Tensor,Tensor&gt; slogdet() const;
  Tensor smm(const Tensor &amp; mat2) const;
  Tensor softmax(int64_t dim, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  std::vector&lt;Tensor&gt; split(int64_t split_size, int64_t dim=0) const;
  std::vector&lt;Tensor&gt; split_with_sizes(IntArrayRef split_sizes, int64_t dim=0) const;
  Tensor squeeze() const;
  Tensor squeeze(int64_t dim) const;
  Tensor &amp; squeeze_();
  Tensor &amp; squeeze_(int64_t dim);
  Tensor sspaddmm(const Tensor &amp; mat1, const Tensor &amp; mat2, Scalar beta=1, Scalar alpha=1) const;
  Tensor stft(int64_t n_fft, c10::optional&lt;int64_t&gt; hop_length=c10::nullopt, c10::optional&lt;int64_t&gt; win_length=c10::nullopt, const Tensor &amp; window={}, bool normalized=false, bool onesided=true) const;
  int64_t stride(int64_t dim) const;
  #ifdef BUILD_NAMEDTENSOR
  int64_t stride(Dimname dim) const;
  #endif
  Tensor sum(c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor sum(IntArrayRef dim, bool keepdim=false, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  #ifdef BUILD_NAMEDTENSOR
  Tensor sum(DimnameList dim, bool keepdim=false, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  #endif
  Tensor sum_to_size(IntArrayRef size) const;
  Tensor sqrt() const;
  Tensor &amp; sqrt_();
  Tensor std(bool unbiased=true) const;
  Tensor std(IntArrayRef dim, bool unbiased=true, bool keepdim=false) const;
  Tensor prod(c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor prod(int64_t dim, bool keepdim=false, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  #ifdef BUILD_NAMEDTENSOR
  Tensor prod(Dimname dim, bool keepdim=false, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  #endif
  Tensor t() const;
  Tensor &amp; t_();
  Tensor tan() const;
  Tensor &amp; tan_();
  Tensor tanh() const;
  Tensor &amp; tanh_();
  Tensor transpose(int64_t dim0, int64_t dim1) const;
  Tensor &amp; transpose_(int64_t dim0, int64_t dim1);
  Tensor flip(IntArrayRef dims) const;
  Tensor roll(IntArrayRef shifts, IntArrayRef dims={}) const;
  Tensor rot90(int64_t k=1, IntArrayRef dims={0,1}) const;
  Tensor trunc() const;
  Tensor &amp; trunc_();
  Tensor type_as(const Tensor &amp; other) const;
  Tensor unsqueeze(int64_t dim) const;
  Tensor &amp; unsqueeze_(int64_t dim);
  Tensor var(bool unbiased=true) const;
  Tensor var(IntArrayRef dim, bool unbiased=true, bool keepdim=false) const;
  Tensor view_as(const Tensor &amp; other) const;
  Tensor where(const Tensor &amp; condition, const Tensor &amp; other) const;
  Tensor norm(c10::optional&lt;Scalar&gt; p, ScalarType dtype) const;
  Tensor norm(Scalar p=2) const;
  Tensor norm(c10::optional&lt;Scalar&gt; p, IntArrayRef dim, bool keepdim, ScalarType dtype) const;
  Tensor norm(c10::optional&lt;Scalar&gt; p, IntArrayRef dim, bool keepdim=false) const;
  Tensor clone() const;
  Tensor &amp; resize_as_(const Tensor &amp; the_template);
  Tensor pow(Scalar exponent) const;
  Tensor &amp; zero_();
  Tensor sub(const Tensor &amp; other, Scalar alpha=1) const;
  Tensor &amp; sub_(const Tensor &amp; other, Scalar alpha=1);
  Tensor sub(Scalar other, Scalar alpha=1) const;
  Tensor &amp; sub_(Scalar other, Scalar alpha=1);
  Tensor addmm(const Tensor &amp; mat1, const Tensor &amp; mat2, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; addmm_(const Tensor &amp; mat1, const Tensor &amp; mat2, Scalar beta=1, Scalar alpha=1);
  Tensor &amp; sparse_resize_(IntArrayRef size, int64_t sparse_dim, int64_t dense_dim);
  Tensor &amp; sparse_resize_and_clear_(IntArrayRef size, int64_t sparse_dim, int64_t dense_dim);
  Tensor sparse_mask(const Tensor &amp; mask) const;
  Tensor to_dense() const;
  int64_t sparse_dim() const;
  int64_t _dimI() const;
  int64_t dense_dim() const;
  int64_t _dimV() const;
  int64_t _nnz() const;
  Tensor coalesce() const;
  bool is_coalesced() const;
  Tensor _indices() const;
  Tensor _values() const;
  Tensor &amp; _coalesced_(bool coalesced);
  Tensor indices() const;
  Tensor values() const;
  int64_t numel() const;
  std::vector&lt;Tensor&gt; unbind(int64_t dim=0) const;
  Tensor to_sparse(int64_t sparse_dim) const;
  Tensor to_sparse() const;
  Tensor to_mkldnn() const;
  Tensor dequantize() const;
  double q_scale() const;
  int64_t q_zero_point() const;
  Tensor int_repr() const;
  QScheme qscheme() const;
  Tensor to(const TensorOptions &amp; options, bool non_blocking=false, bool copy=false) const;
  Tensor to(Device device, ScalarType dtype, bool non_blocking=false, bool copy=false) const;
  Tensor to(ScalarType dtype, bool non_blocking=false, bool copy=false) const;
  Tensor to(const Tensor &amp; other, bool non_blocking=false, bool copy=false) const;
  Scalar item() const;
  Tensor &amp; set_(Storage source);
  Tensor &amp; set_(Storage source, int64_t storage_offset, IntArrayRef size, IntArrayRef stride={});
  Tensor &amp; set_(const Tensor &amp; source);
  Tensor &amp; set_();
  Tensor &amp; set_quantizer_(ConstQuantizerPtr quantizer);
  bool is_set_to(const Tensor &amp; tensor) const;
  Tensor &amp; masked_fill_(const Tensor &amp; mask, Scalar value);
  Tensor masked_fill(const Tensor &amp; mask, Scalar value) const;
  Tensor &amp; masked_fill_(const Tensor &amp; mask, const Tensor &amp; value);
  Tensor masked_fill(const Tensor &amp; mask, const Tensor &amp; value) const;
  Tensor &amp; masked_scatter_(const Tensor &amp; mask, const Tensor &amp; source);
  Tensor masked_scatter(const Tensor &amp; mask, const Tensor &amp; source) const;
  Tensor view(IntArrayRef size) const;
  Tensor &amp; put_(const Tensor &amp; index, const Tensor &amp; source, bool accumulate=false);
  Tensor &amp; index_add_(int64_t dim, const Tensor &amp; index, const Tensor &amp; source);
  Tensor index_add(int64_t dim, const Tensor &amp; index, const Tensor &amp; source) const;
  Tensor &amp; index_fill_(int64_t dim, const Tensor &amp; index, Scalar value);
  Tensor index_fill(int64_t dim, const Tensor &amp; index, Scalar value) const;
  Tensor &amp; index_fill_(int64_t dim, const Tensor &amp; index, const Tensor &amp; value);
  Tensor index_fill(int64_t dim, const Tensor &amp; index, const Tensor &amp; value) const;
  Tensor &amp; scatter_(int64_t dim, const Tensor &amp; index, const Tensor &amp; src);
  Tensor scatter(int64_t dim, const Tensor &amp; index, const Tensor &amp; src) const;
  Tensor &amp; scatter_(int64_t dim, const Tensor &amp; index, Scalar value);
  Tensor scatter(int64_t dim, const Tensor &amp; index, Scalar value) const;
  Tensor &amp; scatter_add_(int64_t dim, const Tensor &amp; index, const Tensor &amp; src);
  Tensor scatter_add(int64_t dim, const Tensor &amp; index, const Tensor &amp; src) const;
  Tensor &amp; lt_(Scalar other);
  Tensor &amp; lt_(const Tensor &amp; other);
  Tensor &amp; gt_(Scalar other);
  Tensor &amp; gt_(const Tensor &amp; other);
  Tensor &amp; le_(Scalar other);
  Tensor &amp; le_(const Tensor &amp; other);
  Tensor &amp; ge_(Scalar other);
  Tensor &amp; ge_(const Tensor &amp; other);
  Tensor &amp; eq_(Scalar other);
  Tensor &amp; eq_(const Tensor &amp; other);
  Tensor &amp; ne_(Scalar other);
  Tensor &amp; ne_(const Tensor &amp; other);
  Tensor __and__(Scalar other) const;
  Tensor __and__(const Tensor &amp; other) const;
  Tensor &amp; __iand__(Scalar other);
  Tensor &amp; __iand__(const Tensor &amp; other);
  Tensor __or__(Scalar other) const;
  Tensor __or__(const Tensor &amp; other) const;
  Tensor &amp; __ior__(Scalar other);
  Tensor &amp; __ior__(const Tensor &amp; other);
  Tensor __xor__(Scalar other) const;
  Tensor __xor__(const Tensor &amp; other) const;
  Tensor &amp; __ixor__(Scalar other);
  Tensor &amp; __ixor__(const Tensor &amp; other);
  Tensor __lshift__(Scalar other) const;
  Tensor __lshift__(const Tensor &amp; other) const;
  Tensor &amp; __ilshift__(Scalar other);
  Tensor &amp; __ilshift__(const Tensor &amp; other);
  Tensor __rshift__(Scalar other) const;
  Tensor __rshift__(const Tensor &amp; other) const;
  Tensor &amp; __irshift__(Scalar other);
  Tensor &amp; __irshift__(const Tensor &amp; other);
  Tensor &amp; lgamma_();
  Tensor &amp; atan2_(const Tensor &amp; other);
  Tensor &amp; tril_(int64_t diagonal=0);
  Tensor &amp; triu_(int64_t diagonal=0);
  Tensor &amp; digamma_();
  Tensor &amp; polygamma_(int64_t n);
  Tensor &amp; erfinv_();
  Tensor &amp; renorm_(Scalar p, int64_t dim, Scalar maxnorm);
  Tensor &amp; pow_(Scalar exponent);
  Tensor &amp; pow_(const Tensor &amp; exponent);
  Tensor &amp; lerp_(const Tensor &amp; end, Scalar weight);
  Tensor &amp; lerp_(const Tensor &amp; end, const Tensor &amp; weight);
  Tensor &amp; sign_();
  Tensor &amp; fmod_(Scalar other);
  Tensor &amp; fmod_(const Tensor &amp; other);
  Tensor &amp; remainder_(Scalar other);
  Tensor &amp; remainder_(const Tensor &amp; other);
  Tensor &amp; addbmm_(const Tensor &amp; batch1, const Tensor &amp; batch2, Scalar beta=1, Scalar alpha=1);
  Tensor addbmm(const Tensor &amp; batch1, const Tensor &amp; batch2, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; addcdiv_(const Tensor &amp; tensor1, const Tensor &amp; tensor2, Scalar value=1);
  Tensor &amp; random_(int64_t from, int64_t to, Generator * generator=nullptr);
  Tensor &amp; random_(int64_t to, Generator * generator=nullptr);
  Tensor &amp; random_(Generator * generator=nullptr);
  Tensor &amp; uniform_(double from=0, double to=1, Generator * generator=nullptr);
  Tensor &amp; normal_(double mean=0, double std=1, Generator * generator=nullptr);
  Tensor &amp; cauchy_(double median=0, double sigma=1, Generator * generator=nullptr);
  Tensor &amp; log_normal_(double mean=1, double std=2, Generator * generator=nullptr);
  Tensor &amp; exponential_(double lambd=1, Generator * generator=nullptr);
  Tensor &amp; geometric_(double p, Generator * generator=nullptr);
  Tensor diag(int64_t diagonal=0) const;
  Tensor cross(const Tensor &amp; other, c10::optional&lt;int64_t&gt; dim=c10::nullopt) const;
  Tensor triu(int64_t diagonal=0) const;
  Tensor tril(int64_t diagonal=0) const;
  Tensor trace() const;
  Tensor ne(Scalar other) const;
  Tensor ne(const Tensor &amp; other) const;
  Tensor eq(Scalar other) const;
  Tensor eq(const Tensor &amp; other) const;
  Tensor ge(Scalar other) const;
  Tensor ge(const Tensor &amp; other) const;
  Tensor le(Scalar other) const;
  Tensor le(const Tensor &amp; other) const;
  Tensor gt(Scalar other) const;
  Tensor gt(const Tensor &amp; other) const;
  Tensor lt(Scalar other) const;
  Tensor lt(const Tensor &amp; other) const;
  Tensor take(const Tensor &amp; index) const;
  Tensor index_select(int64_t dim, const Tensor &amp; index) const;
  Tensor masked_select(const Tensor &amp; mask) const;
  Tensor nonzero() const;
  std::vector&lt;Tensor&gt; nonzero_numpy() const;
  Tensor gather(int64_t dim, const Tensor &amp; index, bool sparse_grad=false) const;
  Tensor addcmul(const Tensor &amp; tensor1, const Tensor &amp; tensor2, Scalar value=1) const;
  Tensor &amp; addcmul_(const Tensor &amp; tensor1, const Tensor &amp; tensor2, Scalar value=1);
  Tensor addcdiv(const Tensor &amp; tensor1, const Tensor &amp; tensor2, Scalar value=1) const;
  std::tuple&lt;Tensor,Tensor&gt; lstsq(const Tensor &amp; A) const;
  std::tuple&lt;Tensor,Tensor&gt; triangular_solve(const Tensor &amp; A, bool upper=true, bool transpose=false, bool unitriangular=false) const;
  std::tuple&lt;Tensor,Tensor&gt; symeig(bool eigenvectors=false, bool upper=true) const;
  std::tuple&lt;Tensor,Tensor&gt; eig(bool eigenvectors=false) const;
  std::tuple&lt;Tensor,Tensor,Tensor&gt; svd(bool some=true, bool compute_uv=true) const;
  Tensor cholesky(bool upper=false) const;
  Tensor cholesky_solve(const Tensor &amp; input2, bool upper=false) const;
  std::tuple&lt;Tensor,Tensor&gt; solve(const Tensor &amp; A) const;
  Tensor cholesky_inverse(bool upper=false) const;
  std::tuple&lt;Tensor,Tensor&gt; qr(bool some=true) const;
  std::tuple&lt;Tensor,Tensor&gt; geqrf() const;
  Tensor orgqr(const Tensor &amp; input2) const;
  Tensor ormqr(const Tensor &amp; input2, const Tensor &amp; input3, bool left=true, bool transpose=false) const;
  Tensor lu_solve(const Tensor &amp; LU_data, const Tensor &amp; LU_pivots) const;
  Tensor multinomial(int64_t num_samples, bool replacement=false, Generator * generator=nullptr) const;
  Tensor lgamma() const;
  Tensor digamma() const;
  Tensor polygamma(int64_t n) const;
  Tensor erfinv() const;
  Tensor dist(const Tensor &amp; other, Scalar p=2) const;
  Tensor atan2(const Tensor &amp; other) const;
  Tensor lerp(const Tensor &amp; end, Scalar weight) const;
  Tensor lerp(const Tensor &amp; end, const Tensor &amp; weight) const;
  Tensor histc(int64_t bins=100, Scalar min=0, Scalar max=0) const;
  Tensor sign() const;
  Tensor fmod(Scalar other) const;
  Tensor fmod(const Tensor &amp; other) const;
  Tensor remainder(Scalar other) const;
  Tensor remainder(const Tensor &amp; other) const;
  Tensor min(const Tensor &amp; other) const;
  Tensor min() const;
  Tensor max(const Tensor &amp; other) const;
  Tensor max() const;
  Tensor median() const;
  std::tuple&lt;Tensor,Tensor&gt; sort(int64_t dim=-1, bool descending=false) const;
  Tensor argsort(int64_t dim=-1, bool descending=false) const;
  std::tuple&lt;Tensor,Tensor&gt; topk(int64_t k, int64_t dim=-1, bool largest=true, bool sorted=true) const;
  Tensor all() const;
  Tensor any() const;
  Tensor renorm(Scalar p, int64_t dim, Scalar maxnorm) const;
  Tensor unfold(int64_t dimension, int64_t size, int64_t step) const;
  bool equal(const Tensor &amp; other) const;
  Tensor pow(const Tensor &amp; exponent) const;
  Tensor alias() const;

  // We changed .dtype() to return a TypeMeta in #12766. Ideally, we want the
  // at::kDouble and its friends to be TypeMeta&#39;s, but that hasn&#39;t happened yet.
  // Before that change, we make this method to maintain BC for C++ usage like
  // `x.to(y.dtype)`.
  // TODO: remove following two after at::kDouble and its friends are TypeMeta&#39;s.
  inline Tensor to(caffe2::TypeMeta type_meta, bool non_blocking=false, bool copy=false) const {
    return this-&gt;to(/*scalar_type=*/typeMetaToScalarType(type_meta), non_blocking, copy);
  }
  inline Tensor to(Device device, caffe2::TypeMeta type_meta, bool non_blocking=false, bool copy=false) const {
    return this-&gt;to(device, /*scalar_type=*/typeMetaToScalarType(type_meta), non_blocking, copy);
  }

  template &lt;typename F, typename... Args&gt;
  auto m(F func, Args&amp;&amp;... params) const -&gt; decltype(func(*this, std::forward&lt;Args&gt;(params)...)) {
    return func(*this, std::forward&lt;Args&gt;(params)...);
  }

protected:
  friend class ::caffe2::Tensor;

  void enforce_invariants();
  c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt; impl_;
};

namespace detail {
// Helper creator for Tensor clas which doesn&#39;t requires the users to pass
// in an intrusive_ptr instead it just converts the argument passed to
// requested intrusive_ptr type.
template &lt;typename T, typename... Args&gt;
Tensor make_tensor(Args&amp;&amp;... args) {
  return Tensor(c10::make_intrusive&lt;T&gt;(std::forward&lt;Args&gt;(args)...));
}
} // namespace detail

} // namespace at

#include &lt;ATen/core/TensorMethods.h&gt;
</pre></div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Program Listing for File Tensor.h</a></li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../_static/jquery.js"></script>
         <script type="text/javascript" src="../_static/underscore.js"></script>
         <script type="text/javascript" src="../_static/doctools.js"></script>
         <script type="text/javascript" src="../_static/language_data.js"></script>
         <script type="text/javascript" src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
         <script type="text/javascript" src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>