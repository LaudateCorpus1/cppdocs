


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Program Listing for File TensorBody.h &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/api/program_listing_file_build_aten_src_ATen_core_TensorBody.h.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/cpp_theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <div class="ecosystem-dropdown">
              <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
                Ecosystem
              </a>
              <div class="ecosystem-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/hub"">
                  <span class=dropdown-title>Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class=dropdown-title>Tools & Libraries</span>
                  <p>Explore the ecosystem of tools and libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <div class="resources-dropdown">
              <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/resources"">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class=dropdown-title>About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../installing.html">Installing C++ Distributions of PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../frontend.html">The C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="library_root.html">Library API</a></li>
</ul>
<p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notes/faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_basics.html">Tensor Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_creation.html">Tensor Creation API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_indexing.html">Tensor Indexing API</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Program Listing for File TensorBody.h</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            
              <!-- User defined GitHub URL -->
              <a href="https://github.com/pytorch/pytorch" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="program-listing-for-file-tensorbody-h">
<span id="program-listing-file-build-aten-src-aten-core-tensorbody-h"></span><h1>Program Listing for File TensorBody.h<a class="headerlink" href="#program-listing-for-file-tensorbody-h" title="Permalink to this headline">¶</a></h1>
<p>↰ <a class="reference internal" href="file_build_aten_src_ATen_core_TensorBody.h.html#file-build-aten-src-aten-core-tensorbody-h"><span class="std std-ref">Return to documentation for file</span></a> (<code class="docutils literal notranslate"><span class="pre">build/aten/src/ATen/core/TensorBody.h</span></code>)</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>#pragma once

#include &lt;c10/core/Device.h&gt;
#include &lt;c10/core/Layout.h&gt;
#include &lt;c10/core/MemoryFormat.h&gt;
#include &lt;c10/core/QScheme.h&gt;
#include &lt;c10/core/Scalar.h&gt;
#include &lt;c10/core/ScalarType.h&gt;
#include &lt;c10/core/Storage.h&gt;
#include &lt;ATen/core/TensorAccessor.h&gt;
#include &lt;c10/core/TensorImpl.h&gt;
#include &lt;c10/core/UndefinedTensorImpl.h&gt;
#include &lt;c10/util/Exception.h&gt;
#include &lt;c10/util/Deprecated.h&gt;
#include &lt;c10/util/Optional.h&gt;
#include &lt;c10/util/intrusive_ptr.h&gt;
#include &lt;ATen/core/DeprecatedTypePropertiesRegistry.h&gt;
#include &lt;ATen/core/DeprecatedTypeProperties.h&gt;
#include &lt;ATen/core/NamedTensor.h&gt;
#include &lt;ATen/core/QuantizerBase.h&gt;
#include &lt;torch/csrc/WindowsTorchApiMacro.h&gt;

namespace caffe2 {
class Tensor;
}
namespace c10{
struct TensorOptions;
}
namespace at {
struct Generator;
struct Type;
class DeprecatedTypeProperties;
class Tensor;
} // namespace at
namespace at {
namespace indexing {
struct TensorIndex;
} // namespace indexing
} // namespace at

namespace torch { namespace autograd {

struct Node;

}} // namespace torch::autograd

namespace at {

class Tensor;
using TensorList = ArrayRef&lt;Tensor&gt;;

namespace impl {
inline bool variable_excluded_from_dispatch() {
#ifdef C10_MOBILE
  // Please read the comment in `VariableFallbackKernel.cpp` about the background of this change.
  return true;
#else
  return c10::impl::tls_local_dispatch_key_set().excluded_.has(DispatchKey::Autograd);
#endif
}
}

// Tensor is a &quot;generic&quot; object holding a pointer to the underlying TensorImpl object, which
// has an embedded reference count. In this way, Tensor is similar to boost::intrusive_ptr.
//
// For example:
//
// void func(Tensor a) {
//   Tensor b = a;
//   ...
// }
//
// In this example, when we say Tensor b = a, we are creating a new object that points to the
// same underlying TensorImpl, and bumps its reference count. When b goes out of scope, the
// destructor decrements the reference count by calling release() on the TensorImpl it points to.
// The existing constructors, operator overloads, etc. take care to implement the correct semantics.
//
// Note that Tensor can also be NULL, i.e. it is not associated with any underlying TensorImpl, and
// special care must be taken to handle this.
class CAFFE2_API Tensor {
 public:
  Tensor(){};
  // This constructor should not be used by end users and is an implementation
  // detail invoked by autogenerated code.
  explicit Tensor(
      c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt; tensor_impl)
      : impl_(std::move(tensor_impl)) {
    if (impl_.get() == nullptr) {
      throw std::runtime_error(&quot;TensorImpl with nullptr is not supported&quot;);
    }
  }
  Tensor(const Tensor&amp;) = default;
  Tensor(Tensor&amp;&amp;) = default;


 public:
  // Creates a new wrapper from TensorImpl. Intentionally a free method because
  // it should be used with care. Checks necessary invariants
  static Tensor wrap_tensor_impl(
      c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt; tensor_impl) {
    Tensor r(std::move(tensor_impl));
    r.enforce_invariants();
    return r;
  }

  int64_t dim() const {
    return impl_-&gt;dim();
  }
  int64_t storage_offset() const {
    return impl_-&gt;storage_offset();
  }

  TensorImpl * unsafeGetTensorImpl() const {
    return impl_.get();
  }
  TensorImpl * unsafeReleaseTensorImpl() {
    return impl_.release();
  }
  const c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt;&amp; getIntrusivePtr() const {
    return impl_;
  }

  bool defined() const {
    return impl_;
  }

  void reset() {
    impl_.reset();
  }

  // The following overloads are very intruiging.  Consider the following
  // program:
  //
  //    x[1] = 3;
  //
  // We would expect that the first entry of x is written to 3.  But how can we
  // actually achieve this?  x[1] evaluates to a tensor...
  //
  // The answer is, using a ref-qualifier.  x[1] is an rvalue, which cannot be
  // (profitably) assigned to in the traditional sense, so we overload
  // assignment to mean, &quot;Actually, copy 3 into the tensor data.&quot;  This is done
  // with an rvalue-reference ref-qualified overload (the methods with &amp;&amp; at the
  // end of their type.)
  //
  // There&#39;s one more fly in the ointment: We also want
  //
  //    Tensor x = y;
  //
  // to work, and we want it NOT to copy.  So we need a traditional operator=
  // overload.  But we MUST specify a mutable lvalue ref-qualifier, to
  // disambiguate the traditional overload from the rvalue-reference
  // ref-qualified overload.  Otherwise, it will be ambiguous, because
  // a non ref-qualified method is eligible for all situations.

  // Unfortunately, we have to write these constructors out manually
  // to work around an MSVC bug:
  //    error C2580: &#39;at::Tensor &amp;at::Tensor::operator =(const at::Tensor &amp;) &amp;&#39;:
  //    multiple versions of a defaulted special member functions are not allowed
  // Tensor&amp; operator=(const Tensor&amp;) &amp; = default;
  // Tensor&amp; operator=(Tensor&amp;&amp;) &amp; = default;

  // Also MSVC will wrongly issue the following warning with the aforementioned fix
  //    warning C4522: &#39;at::Tensor&#39;: multiple assignment operators specified
  // Let&#39;s just skip the warning.

  #ifdef _MSC_VER
  #pragma warning( push )
  #pragma warning( disable : 4522 )
  #endif

  Tensor&amp; operator=(const Tensor&amp; x) &amp; {
    impl_ = x.impl_;
    return *this;
  }
  Tensor&amp; operator=(Tensor&amp;&amp; x) &amp; {
    impl_ = std::move(x.impl_);
    return *this;
  }

  Tensor&amp; operator=(Scalar v) &amp;&amp;;
  Tensor&amp; operator=(const Tensor&amp;) &amp;&amp;;
  Tensor&amp; operator=(Tensor&amp;&amp;) &amp;&amp;;

  #ifdef _MSC_VER
  #pragma warning( pop )
  #endif

  bool is_same(const Tensor&amp; other) const noexcept {
    return impl_ == other.impl_;
  }
  size_t use_count() const noexcept {
    return impl_.use_count();
  }
  size_t weak_use_count() const noexcept {
    return impl_.weak_use_count();
  }

  std::string toString() const;

  IntArrayRef sizes() const {
    return impl_-&gt;sizes();
  }
  IntArrayRef strides() const {
    return impl_-&gt;strides();
  }
  // See impl::get_opt_names in ATen/NamedTensor.h for docs.
  optional&lt;DimnameList&gt; opt_names() const {
    return impl::get_opt_names(unsafeGetTensorImpl());
  }
  // See impl::get_names in ATen/NamedTensor.h for docs.
  DimnameList names() const {
    return impl::get_names(unsafeGetTensorImpl());
  }
  int64_t ndimension() const {
    return dim();
  }

  bool is_contiguous(at::MemoryFormat memory_format=at::MemoryFormat::Contiguous) const {
    return impl_-&gt;is_contiguous(memory_format);
  }

  bool is_non_overlapping_and_dense() const {
    return impl_-&gt;is_non_overlapping_and_dense();
  }

  at::MemoryFormat suggest_memory_format(
      bool channels_last_strides_exact_match = false) const {
    // Setting channels_last_strides_exact_match to true forces function to
    // check 0,1 - sized dimension strides.
    if (!is_mkldnn() &amp;&amp; !is_sparse()) {
      if (impl_-&gt;is_strides_like_channels_last()) {
        if (!channels_last_strides_exact_match ||
            get_channels_last_strides_2d(sizes()) == strides()) {
          return at::MemoryFormat::ChannelsLast;
        }
      }
      else if (impl_-&gt;is_strides_like_channels_last_3d()) {
        if (!channels_last_strides_exact_match ||
            get_channels_last_strides_3d(sizes()) == strides()) {
          return at::MemoryFormat::ChannelsLast3d;
        }
      }
    }
    return at::MemoryFormat::Contiguous;
  }

  // Total bytes consumed by the &quot;view&quot; of elements of the array.  Does not
  // include size of metadata.  The number reported here does not necessarily
  // correspond to the true physical memory consumed by a tensor; instead,
  // it reports the memory the tensor would take *if* it were contiguous.
  // Defined to be numel() * itemsize()
  size_t nbytes() const {
    TORCH_CHECK(layout () != at::kSparse,
                &quot;nbytes is not defined for sparse tensors.  If you want the size of the constituent &quot; \
                &quot;tensors, add the nbytes of the indices and values.  If you want the size of the  &quot; \
                &quot;equivalent dense tensor, multiply numel() by element_size()&quot;);
    return impl_-&gt;numel() * impl_-&gt;itemsize();
  }

  int64_t numel() const {
    return impl_-&gt;numel();
  }

  // Length of one array element in bytes.  This is the traditional
  // Numpy naming.
  size_t itemsize() const {
    return impl_-&gt;itemsize();
  }

  // Same as itemsize().  This is the PyTorch naming.
  int64_t element_size() const {
    return static_cast&lt;int64_t&gt;(impl_-&gt;itemsize());
  }

  C10_DEPRECATED_MESSAGE(&quot;Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device().&quot;)
  DeprecatedTypeProperties &amp; type() const {
    return globalDeprecatedTypePropertiesRegistry().getDeprecatedTypeProperties(
        dispatchKeyToBackend(legacyExtractDispatchKey(key_set())),
        scalar_type());
  }
  DispatchKeySet key_set() const {
    return impl_-&gt;key_set();
  }
  ScalarType scalar_type() const {
    return typeMetaToScalarType(impl_-&gt;dtype());
  }
  bool has_storage() const {
    return defined() &amp;&amp; impl_-&gt;has_storage();
  }
  const Storage&amp; storage() const {
    return impl_-&gt;storage();
  }
  bool is_alias_of(const at::Tensor&amp; other) const{
    return impl_-&gt;storage().is_alias_of(other.storage());
  }
  Tensor toType(ScalarType t) const;
  Tensor toBackend(Backend b) const;

  C10_DEPRECATED_MESSAGE(&quot;Tensor.is_variable() is deprecated; everything is a variable now. (If you want to assert that variable has been appropriately handled already, use at::impl::variable_excluded_from_dispatch())&quot;)
  bool is_variable() const noexcept {
    return !at::impl::variable_excluded_from_dispatch();
  }

  Layout layout() const noexcept;

  caffe2::TypeMeta dtype() const noexcept;

  Device device() const;

  int64_t get_device() const;

  bool is_cuda() const;

  bool is_hip() const;

  bool is_sparse() const;

  bool is_mkldnn() const;

  bool is_vulkan() const;

  bool is_quantized() const;

  bool is_meta() const;

  QuantizerPtr quantizer() const;

  bool has_names() const;

  const NamedTensorMeta* get_named_tensor_meta() const;
  NamedTensorMeta* get_named_tensor_meta();

  TensorOptions options() const;

  void* data_ptr() const {
    return this-&gt;unsafeGetTensorImpl()-&gt;data();
  }

  template &lt;typename T&gt;
  T * data_ptr() const;

  template&lt;typename T&gt;
  C10_DEPRECATED_MESSAGE(&quot;Tensor.data&lt;T&gt;() is deprecated. Please use Tensor.data_ptr&lt;T&gt;() instead.&quot;)
  T * data() const {
    return data_ptr&lt;T&gt;();
  }

  template &lt;typename T&gt;
  T item() const;

  // Purposely not defined here to avoid inlining
  void print() const;

  // Return a `TensorAccessor` for CPU `Tensor`s. You have to specify scalar type and
  // dimension.
  template&lt;typename T, size_t N&gt;
  TensorAccessor&lt;T,N&gt; accessor() const&amp; {
    static_assert(N &gt; 0, &quot;accessor is used for indexing tensor, for scalars use *data_ptr&lt;T&gt;()&quot;);
    TORCH_CHECK(dim() == N, &quot;expected &quot;, N, &quot; dims but tensor has &quot;, dim());
    return TensorAccessor&lt;T,N&gt;(data_ptr&lt;T&gt;(),sizes().data(),strides().data());
  }
  template&lt;typename T, size_t N&gt;
  TensorAccessor&lt;T,N&gt; accessor() &amp;&amp; = delete;

  // Return a `GenericPackedTensorAccessor` for CUDA `Tensor`s. You have to specify scalar type and
  // dimension. You can optionally specify RestrictPtrTraits as a template parameter to
  // cast the data pointer to a __restrict__ pointer.
  // In order to use this, your CUDA kernel has to take a corresponding GenericPackedTensorAccessor
  // as an argument.
  template&lt;typename T, size_t N, template &lt;typename U&gt; class PtrTraits = DefaultPtrTraits, typename index_t = int64_t&gt;
  GenericPackedTensorAccessor&lt;T,N,PtrTraits,index_t&gt; generic_packed_accessor() const&amp; {
    static_assert(N &gt; 0, &quot;accessor is used for indexing tensor, for scalars use *data_ptr&lt;T&gt;()&quot;);
    TORCH_CHECK(dim() == N, &quot;expected &quot;, N, &quot; dims but tensor has &quot;, dim());
    return GenericPackedTensorAccessor&lt;T,N,PtrTraits,index_t&gt;(static_cast&lt;typename PtrTraits&lt;T&gt;::PtrType&gt;(data_ptr&lt;T&gt;()),sizes().data(),strides().data());
  }
  template&lt;typename T, size_t N, template &lt;typename U&gt; class PtrTraits = DefaultPtrTraits, typename index_t = int64_t&gt;
  GenericPackedTensorAccessor&lt;T,N&gt; generic_packed_accessor() &amp;&amp; = delete;

  template&lt;typename T, size_t N, template &lt;typename U&gt; class PtrTraits = DefaultPtrTraits&gt;
  PackedTensorAccessor32&lt;T,N,PtrTraits&gt; packed_accessor32() const&amp; {
    return generic_packed_accessor&lt;T,N,PtrTraits,int32_t&gt;();
  }
  template&lt;typename T, size_t N, template &lt;typename U&gt; class PtrTraits = DefaultPtrTraits&gt;
  PackedTensorAccessor32&lt;T,N,PtrTraits&gt; packed_accessor32() &amp;&amp; = delete;

  template&lt;typename T, size_t N, template &lt;typename U&gt; class PtrTraits = DefaultPtrTraits&gt;
  PackedTensorAccessor64&lt;T,N,PtrTraits&gt; packed_accessor64() const&amp; {
    return generic_packed_accessor&lt;T,N,PtrTraits,int64_t&gt;();
  }
  template&lt;typename T, size_t N, template &lt;typename U&gt; class PtrTraits = DefaultPtrTraits&gt;
  PackedTensorAccessor64&lt;T,N,PtrTraits&gt; packed_accessor64() &amp;&amp; = delete;

  template&lt;typename T, size_t N, template &lt;typename U&gt; class PtrTraits = DefaultPtrTraits, typename index_t = int64_t&gt;
  C10_DEPRECATED_MESSAGE(&quot;packed_accessor is deprecated, use packed_accessor32 or packed_accessor64 instead&quot;)
  GenericPackedTensorAccessor&lt;T,N,PtrTraits,index_t&gt; packed_accessor() const &amp; {
    return generic_packed_accessor&lt;T,N,PtrTraits,index_t&gt;();
  }
  template&lt;typename T, size_t N, template &lt;typename U&gt; class PtrTraits = DefaultPtrTraits, typename index_t = int64_t&gt;
  C10_DEPRECATED_MESSAGE(&quot;packed_accessor is deprecated, use packed_accessor32 or packed_accessor64 instead&quot;)
  GenericPackedTensorAccessor&lt;T,N,PtrTraits,index_t&gt; packed_accessor() &amp;&amp; = delete;

  Tensor operator~() const;
  Tensor operator-() const;
  Tensor&amp; operator+=(const Tensor &amp; other);
  Tensor&amp; operator+=(Scalar other);
  Tensor&amp; operator-=(const Tensor &amp; other);
  Tensor&amp; operator-=(Scalar other);
  Tensor&amp; operator*=(const Tensor &amp; other);
  Tensor&amp; operator*=(Scalar other);
  Tensor&amp; operator/=(const Tensor &amp; other);
  Tensor&amp; operator/=(Scalar other);
  Tensor&amp; operator&amp;=(const Tensor &amp; other);
  Tensor&amp; operator|=(const Tensor &amp; other);
  Tensor&amp; operator^=(const Tensor &amp; other);
  Tensor operator[](Scalar index) const;
  Tensor operator[](Tensor index) const;
  Tensor operator[](int64_t index) const;

  Tensor index(ArrayRef&lt;at::indexing::TensorIndex&gt; indices) const;
  Tensor index(std::initializer_list&lt;at::indexing::TensorIndex&gt; indices) const;

  Tensor &amp; index_put_(ArrayRef&lt;at::indexing::TensorIndex&gt; indices, Tensor const &amp; rhs);
  Tensor &amp; index_put_(ArrayRef&lt;at::indexing::TensorIndex&gt; indices, Scalar v);
  Tensor &amp; index_put_(std::initializer_list&lt;at::indexing::TensorIndex&gt; indices, Tensor const &amp; rhs);
  Tensor &amp; index_put_(std::initializer_list&lt;at::indexing::TensorIndex&gt; indices, Scalar v);

  Tensor cpu() const;
  Tensor cuda() const;
  Tensor hip() const;
  Tensor vulkan() const;

  // ~~~~~ Autograd API ~~~~~






  Tensor&amp; set_requires_grad(bool requires_grad) {
    impl_-&gt;set_requires_grad(requires_grad);
    return *this;
  }
  bool requires_grad() const {
    return impl_-&gt;requires_grad();
  }

  Tensor&amp; mutable_grad() {
    return impl_-&gt;mutable_grad();
  }

  const Tensor&amp; grad() const {
    return impl_-&gt;grad();
  }

  // STOP.  Thinking of adding a method here, which only makes use
  // of other ATen methods?  Define it in native_functions.yaml.

  //example
  //Tensor * add(Tensor &amp; b);
  void backward(const c10::optional&lt;Tensor&gt;&amp; gradient={}, c10::optional&lt;bool&gt; retain_graph=c10::nullopt, bool create_graph=false) const;
  void set_data(const Tensor &amp; new_data) const;
  Tensor data() const;
  bool is_leaf() const;
  int64_t output_nr() const;
  int64_t _version() const;
  Tensor &amp; requires_grad_(bool requires_grad=true) const;
  void retain_grad() const;
  Tensor &amp; rename_(c10::optional&lt;DimnameList&gt; names) const;
  Tensor rename(c10::optional&lt;DimnameList&gt; names) const;
  Tensor align_to(DimnameList names) const;
  Tensor align_to(DimnameList order, int64_t ellipsis_idx) const;
  Tensor align_as(const Tensor &amp; other) const;
  Tensor refine_names(DimnameList names) const;
  Tensor abs() const;
  Tensor &amp; abs_() const;
  Tensor absolute() const;
  Tensor &amp; absolute_() const;
  Tensor angle() const;
  Tensor conj() const;
  Tensor acos() const;
  Tensor &amp; acos_() const;
  Tensor add(const Tensor &amp; other, Scalar alpha=1) const;
  Tensor &amp; add_(const Tensor &amp; other, Scalar alpha=1) const;
  Tensor add(Scalar other, Scalar alpha=1) const;
  Tensor &amp; add_(Scalar other, Scalar alpha=1) const;
  Tensor addmv(const Tensor &amp; mat, const Tensor &amp; vec, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; addmv_(const Tensor &amp; mat, const Tensor &amp; vec, Scalar beta=1, Scalar alpha=1) const;
  Tensor addr(const Tensor &amp; vec1, const Tensor &amp; vec2, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; addr_(const Tensor &amp; vec1, const Tensor &amp; vec2, Scalar beta=1, Scalar alpha=1) const;
  Tensor all(int64_t dim, bool keepdim=false) const;
  Tensor all(Dimname dim, bool keepdim=false) const;
  bool allclose(const Tensor &amp; other, double rtol=1e-05, double atol=1e-08, bool equal_nan=false) const;
  Tensor any(int64_t dim, bool keepdim=false) const;
  Tensor any(Dimname dim, bool keepdim=false) const;
  Tensor argmax(c10::optional&lt;int64_t&gt; dim=c10::nullopt, bool keepdim=false) const;
  Tensor argmin(c10::optional&lt;int64_t&gt; dim=c10::nullopt, bool keepdim=false) const;
  Tensor acosh() const;
  Tensor &amp; acosh_() const;
  Tensor arccosh() const;
  Tensor &amp; arccosh_() const;
  Tensor asinh() const;
  Tensor &amp; asinh_() const;
  Tensor atanh() const;
  Tensor &amp; atanh_() const;
  Tensor as_strided(IntArrayRef size, IntArrayRef stride, c10::optional&lt;int64_t&gt; storage_offset=c10::nullopt) const;
  Tensor &amp; as_strided_(IntArrayRef size, IntArrayRef stride, c10::optional&lt;int64_t&gt; storage_offset=c10::nullopt) const;
  Tensor asin() const;
  Tensor &amp; asin_() const;
  Tensor atan() const;
  Tensor &amp; atan_() const;
  Tensor baddbmm(const Tensor &amp; batch1, const Tensor &amp; batch2, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; baddbmm_(const Tensor &amp; batch1, const Tensor &amp; batch2, Scalar beta=1, Scalar alpha=1) const;
  Tensor bernoulli(c10::optional&lt;Generator&gt; generator=c10::nullopt) const;
  Tensor &amp; bernoulli_(const Tensor &amp; p, c10::optional&lt;Generator&gt; generator=c10::nullopt) const;
  Tensor &amp; bernoulli_(double p=0.5, c10::optional&lt;Generator&gt; generator=c10::nullopt) const;
  Tensor bernoulli(double p, c10::optional&lt;Generator&gt; generator=c10::nullopt) const;
  Tensor bincount(const c10::optional&lt;Tensor&gt;&amp; weights={}, int64_t minlength=0) const;
  Tensor bitwise_not() const;
  Tensor &amp; bitwise_not_() const;
  Tensor logical_not() const;
  Tensor &amp; logical_not_() const;
  Tensor logical_xor(const Tensor &amp; other) const;
  Tensor &amp; logical_xor_(const Tensor &amp; other) const;
  Tensor logical_and(const Tensor &amp; other) const;
  Tensor &amp; logical_and_(const Tensor &amp; other) const;
  Tensor logical_or(const Tensor &amp; other) const;
  Tensor &amp; logical_or_(const Tensor &amp; other) const;
  Tensor bmm(const Tensor &amp; mat2) const;
  Tensor ceil() const;
  Tensor &amp; ceil_() const;
  std::vector&lt;Tensor&gt; unsafe_chunk(int64_t chunks, int64_t dim=0) const;
  std::vector&lt;Tensor&gt; chunk(int64_t chunks, int64_t dim=0) const;
  Tensor clamp(c10::optional&lt;Scalar&gt; min=c10::nullopt, c10::optional&lt;Scalar&gt; max=c10::nullopt) const;
  Tensor &amp; clamp_(c10::optional&lt;Scalar&gt; min=c10::nullopt, c10::optional&lt;Scalar&gt; max=c10::nullopt) const;
  Tensor clamp_max(Scalar max) const;
  Tensor &amp; clamp_max_(Scalar max) const;
  Tensor clamp_min(Scalar min) const;
  Tensor &amp; clamp_min_(Scalar min) const;
  Tensor clip(c10::optional&lt;Scalar&gt; min=c10::nullopt, c10::optional&lt;Scalar&gt; max=c10::nullopt) const;
  Tensor &amp; clip_(c10::optional&lt;Scalar&gt; min=c10::nullopt, c10::optional&lt;Scalar&gt; max=c10::nullopt) const;
  Tensor contiguous(MemoryFormat memory_format=MemoryFormat::Contiguous) const;
  Tensor &amp; copy_(const Tensor &amp; src, bool non_blocking=false) const;
  Tensor cos() const;
  Tensor &amp; cos_() const;
  Tensor cosh() const;
  Tensor &amp; cosh_() const;
  Tensor count_nonzero(IntArrayRef dim) const;
  Tensor count_nonzero(c10::optional&lt;int64_t&gt; dim=c10::nullopt) const;
  std::tuple&lt;Tensor,Tensor&gt; cummax(int64_t dim) const;
  std::tuple&lt;Tensor,Tensor&gt; cummax(Dimname dim) const;
  std::tuple&lt;Tensor,Tensor&gt; cummin(int64_t dim) const;
  std::tuple&lt;Tensor,Tensor&gt; cummin(Dimname dim) const;
  Tensor cumprod(int64_t dim, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor cumprod(Dimname dim, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor cumsum(int64_t dim, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor cumsum(Dimname dim, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor diag_embed(int64_t offset=0, int64_t dim1=-2, int64_t dim2=-1) const;
  Tensor diagflat(int64_t offset=0) const;
  Tensor diagonal(int64_t offset=0, int64_t dim1=0, int64_t dim2=1) const;
  Tensor diagonal(Dimname outdim, Dimname dim1, Dimname dim2, int64_t offset=0) const;
  Tensor &amp; fill_diagonal_(Scalar fill_value, bool wrap=false) const;
  Tensor div(const Tensor &amp; other) const;
  Tensor &amp; div_(const Tensor &amp; other) const;
  Tensor div(Scalar other) const;
  Tensor &amp; div_(Scalar other) const;
  Tensor dot(const Tensor &amp; tensor) const;
  Tensor new_empty(IntArrayRef size, const TensorOptions &amp; options={}) const;
  Tensor new_full(IntArrayRef size, Scalar fill_value, const TensorOptions &amp; options={}) const;
  Tensor new_zeros(IntArrayRef size, const TensorOptions &amp; options={}) const;
  Tensor &amp; resize_(IntArrayRef size, c10::optional&lt;MemoryFormat&gt; memory_format=c10::nullopt) const;
  Tensor erf() const;
  Tensor &amp; erf_() const;
  Tensor erfc() const;
  Tensor &amp; erfc_() const;
  Tensor exp() const;
  Tensor &amp; exp_() const;
  Tensor expm1() const;
  Tensor &amp; expm1_() const;
  Tensor expand(IntArrayRef size, bool implicit=false) const;
  Tensor expand_as(const Tensor &amp; other) const;
  Tensor flatten(int64_t start_dim=0, int64_t end_dim=-1) const;
  Tensor flatten(int64_t start_dim, int64_t end_dim, Dimname out_dim) const;
  Tensor flatten(Dimname start_dim, Dimname end_dim, Dimname out_dim) const;
  Tensor flatten(DimnameList dims, Dimname out_dim) const;
  Tensor unflatten(int64_t dim, IntArrayRef sizes, c10::optional&lt;DimnameList&gt; names=c10::nullopt) const;
  Tensor unflatten(Dimname dim, IntArrayRef sizes, DimnameList names) const;
  Tensor &amp; fill_(Scalar value) const;
  Tensor &amp; fill_(const Tensor &amp; value) const;
  Tensor floor() const;
  Tensor &amp; floor_() const;
  Tensor floor_divide(const Tensor &amp; other) const;
  Tensor &amp; floor_divide_(const Tensor &amp; other) const;
  Tensor floor_divide(Scalar other) const;
  Tensor &amp; floor_divide_(Scalar other) const;
  Tensor frac() const;
  Tensor &amp; frac_() const;
  Tensor gcd(const Tensor &amp; other) const;
  Tensor &amp; gcd_(const Tensor &amp; other) const;
  Tensor lcm(const Tensor &amp; other) const;
  Tensor &amp; lcm_(const Tensor &amp; other) const;
  Tensor ifft(int64_t signal_ndim, bool normalized=false) const;
  Tensor rfft(int64_t signal_ndim, bool normalized=false, bool onesided=true) const;
  Tensor irfft(int64_t signal_ndim, bool normalized=false, bool onesided=true, IntArrayRef signal_sizes={}) const;
  Tensor index(TensorList indices) const;
  Tensor &amp; index_copy_(int64_t dim, const Tensor &amp; index, const Tensor &amp; source) const;
  Tensor index_copy(int64_t dim, const Tensor &amp; index, const Tensor &amp; source) const;
  Tensor &amp; index_copy_(Dimname dim, const Tensor &amp; index, const Tensor &amp; source) const;
  Tensor index_copy(Dimname dim, const Tensor &amp; index, const Tensor &amp; source) const;
  Tensor &amp; index_put_(TensorList indices, const Tensor &amp; values, bool accumulate=false) const;
  Tensor index_put(TensorList indices, const Tensor &amp; values, bool accumulate=false) const;
  Tensor inverse() const;
  Tensor isclose(const Tensor &amp; other, double rtol=1e-05, double atol=1e-08, bool equal_nan=false) const;
  Tensor isnan() const;
  bool is_distributed() const;
  bool is_floating_point() const;
  bool is_complex() const;
  Tensor isreal() const;
  bool is_nonzero() const;
  bool is_same_size(const Tensor &amp; other) const;
  bool is_signed() const;
  std::tuple&lt;Tensor,Tensor&gt; kthvalue(int64_t k, int64_t dim=-1, bool keepdim=false) const;
  std::tuple&lt;Tensor,Tensor&gt; kthvalue(int64_t k, Dimname dim, bool keepdim=false) const;
  Tensor log() const;
  Tensor &amp; log_() const;
  Tensor log10() const;
  Tensor &amp; log10_() const;
  Tensor log1p() const;
  Tensor &amp; log1p_() const;
  Tensor log2() const;
  Tensor &amp; log2_() const;
  Tensor logaddexp(const Tensor &amp; other) const;
  Tensor logaddexp2(const Tensor &amp; other) const;
  Tensor logdet() const;
  Tensor log_softmax(int64_t dim, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor log_softmax(Dimname dim, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor logcumsumexp(int64_t dim) const;
  Tensor logcumsumexp(Dimname dim) const;
  Tensor logsumexp(IntArrayRef dim, bool keepdim=false) const;
  Tensor logsumexp(DimnameList dim, bool keepdim=false) const;
  Tensor matmul(const Tensor &amp; other) const;
  Tensor matrix_power(int64_t n) const;
  std::tuple&lt;Tensor,Tensor&gt; max(int64_t dim, bool keepdim=false) const;
  Tensor max_values(IntArrayRef dim, bool keepdim=false) const;
  std::tuple&lt;Tensor,Tensor&gt; max(Dimname dim, bool keepdim=false) const;
  Tensor max_values(DimnameList dim, bool keepdim=false) const;
  Tensor mean(c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor mean(IntArrayRef dim, bool keepdim=false, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor mean(DimnameList dim, bool keepdim=false, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  std::tuple&lt;Tensor,Tensor&gt; median(int64_t dim, bool keepdim=false) const;
  std::tuple&lt;Tensor,Tensor&gt; median(Dimname dim, bool keepdim=false) const;
  std::tuple&lt;Tensor,Tensor&gt; min(int64_t dim, bool keepdim=false) const;
  Tensor min_values(IntArrayRef dim, bool keepdim=false) const;
  std::tuple&lt;Tensor,Tensor&gt; min(Dimname dim, bool keepdim=false) const;
  Tensor min_values(DimnameList dim, bool keepdim=false) const;
  Tensor mm(const Tensor &amp; mat2) const;
  std::tuple&lt;Tensor,Tensor&gt; mode(int64_t dim=-1, bool keepdim=false) const;
  std::tuple&lt;Tensor,Tensor&gt; mode(Dimname dim, bool keepdim=false) const;
  Tensor mul(const Tensor &amp; other) const;
  Tensor &amp; mul_(const Tensor &amp; other) const;
  Tensor mul(Scalar other) const;
  Tensor &amp; mul_(Scalar other) const;
  Tensor mv(const Tensor &amp; vec) const;
  Tensor mvlgamma(int64_t p) const;
  Tensor &amp; mvlgamma_(int64_t p) const;
  Tensor narrow_copy(int64_t dim, int64_t start, int64_t length) const;
  Tensor narrow(int64_t dim, int64_t start, int64_t length) const;
  Tensor narrow(int64_t dim, const Tensor &amp; start, int64_t length) const;
  Tensor permute(IntArrayRef dims) const;
  Tensor numpy_T() const;
  bool is_pinned() const;
  Tensor pin_memory() const;
  Tensor pinverse(double rcond=1e-15) const;
  Tensor rad2deg() const;
  Tensor &amp; rad2deg_() const;
  Tensor deg2rad() const;
  Tensor &amp; deg2rad_() const;
  Tensor reciprocal() const;
  Tensor &amp; reciprocal_() const;
  Tensor neg() const;
  Tensor &amp; neg_() const;
  Tensor repeat(IntArrayRef repeats) const;
  Tensor repeat_interleave(const Tensor &amp; repeats, c10::optional&lt;int64_t&gt; dim=c10::nullopt) const;
  Tensor repeat_interleave(int64_t repeats, c10::optional&lt;int64_t&gt; dim=c10::nullopt) const;
  Tensor reshape(IntArrayRef shape) const;
  Tensor reshape_as(const Tensor &amp; other) const;
  Tensor round() const;
  Tensor &amp; round_() const;
  Tensor relu() const;
  Tensor &amp; relu_() const;
  Tensor prelu(const Tensor &amp; weight) const;
  std::tuple&lt;Tensor,Tensor&gt; prelu_backward(const Tensor &amp; grad_output, const Tensor &amp; weight) const;
  Tensor hardshrink(Scalar lambd=0.5) const;
  Tensor hardshrink_backward(const Tensor &amp; grad_out, Scalar lambd) const;
  Tensor rsqrt() const;
  Tensor &amp; rsqrt_() const;
  Tensor select(Dimname dim, int64_t index) const;
  Tensor select(int64_t dim, int64_t index) const;
  Tensor sigmoid() const;
  Tensor &amp; sigmoid_() const;
  Tensor logit(c10::optional&lt;double&gt; eps=c10::nullopt) const;
  Tensor &amp; logit_(c10::optional&lt;double&gt; eps=c10::nullopt) const;
  Tensor sin() const;
  Tensor &amp; sin_() const;
  Tensor sinh() const;
  Tensor &amp; sinh_() const;
  Tensor detach() const;
  Tensor &amp; detach_() const;
  int64_t size(int64_t dim) const;
  int64_t size(Dimname dim) const;
  Tensor slice(int64_t dim=0, int64_t start=0, int64_t end=9223372036854775807, int64_t step=1) const;
  std::tuple&lt;Tensor,Tensor&gt; slogdet() const;
  Tensor smm(const Tensor &amp; mat2) const;
  Tensor softmax(int64_t dim, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor softmax(Dimname dim, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  std::vector&lt;Tensor&gt; unsafe_split(int64_t split_size, int64_t dim=0) const;
  std::vector&lt;Tensor&gt; split(int64_t split_size, int64_t dim=0) const;
  std::vector&lt;Tensor&gt; unsafe_split_with_sizes(IntArrayRef split_sizes, int64_t dim=0) const;
  std::vector&lt;Tensor&gt; split_with_sizes(IntArrayRef split_sizes, int64_t dim=0) const;
  Tensor squeeze() const;
  Tensor squeeze(int64_t dim) const;
  Tensor squeeze(Dimname dim) const;
  Tensor &amp; squeeze_() const;
  Tensor &amp; squeeze_(int64_t dim) const;
  Tensor &amp; squeeze_(Dimname dim) const;
  Tensor sspaddmm(const Tensor &amp; mat1, const Tensor &amp; mat2, Scalar beta=1, Scalar alpha=1) const;
  Tensor stft(int64_t n_fft, c10::optional&lt;int64_t&gt; hop_length=c10::nullopt, c10::optional&lt;int64_t&gt; win_length=c10::nullopt, const c10::optional&lt;Tensor&gt;&amp; window={}, bool normalized=false, bool onesided=true) const;
  Tensor istft(int64_t n_fft, c10::optional&lt;int64_t&gt; hop_length=c10::nullopt, c10::optional&lt;int64_t&gt; win_length=c10::nullopt, const c10::optional&lt;Tensor&gt;&amp; window={}, bool center=true, bool normalized=false, bool onesided=true, c10::optional&lt;int64_t&gt; length=c10::nullopt) const;
  int64_t stride(int64_t dim) const;
  int64_t stride(Dimname dim) const;
  Tensor sum(c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor sum(IntArrayRef dim, bool keepdim=false, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor sum(DimnameList dim, bool keepdim=false, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor nansum(c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor nansum(IntArrayRef dim, bool keepdim=false, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor sum_to_size(IntArrayRef size) const;
  Tensor sqrt() const;
  Tensor &amp; sqrt_() const;
  Tensor square() const;
  Tensor &amp; square_() const;
  Tensor std(bool unbiased=true) const;
  Tensor std(IntArrayRef dim, bool unbiased=true, bool keepdim=false) const;
  Tensor std(DimnameList dim, bool unbiased=true, bool keepdim=false) const;
  Tensor prod(c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor prod(int64_t dim, bool keepdim=false, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor prod(Dimname dim, bool keepdim=false, c10::optional&lt;ScalarType&gt; dtype=c10::nullopt) const;
  Tensor t() const;
  Tensor &amp; t_() const;
  Tensor tan() const;
  Tensor &amp; tan_() const;
  Tensor tanh() const;
  Tensor &amp; tanh_() const;
  Tensor transpose(int64_t dim0, int64_t dim1) const;
  Tensor transpose(Dimname dim0, Dimname dim1) const;
  Tensor &amp; transpose_(int64_t dim0, int64_t dim1) const;
  Tensor flip(IntArrayRef dims) const;
  Tensor fliplr() const;
  Tensor flipud() const;
  Tensor roll(IntArrayRef shifts, IntArrayRef dims={}) const;
  Tensor rot90(int64_t k=1, IntArrayRef dims={0,1}) const;
  Tensor true_divide(const Tensor &amp; other) const;
  Tensor &amp; true_divide_(const Tensor &amp; other) const;
  Tensor true_divide(Scalar other) const;
  Tensor &amp; true_divide_(Scalar other) const;
  Tensor trunc() const;
  Tensor &amp; trunc_() const;
  Tensor type_as(const Tensor &amp; other) const;
  Tensor unsqueeze(int64_t dim) const;
  Tensor &amp; unsqueeze_(int64_t dim) const;
  Tensor var(bool unbiased=true) const;
  Tensor var(IntArrayRef dim, bool unbiased=true, bool keepdim=false) const;
  Tensor var(DimnameList dim, bool unbiased=true, bool keepdim=false) const;
  Tensor view_as(const Tensor &amp; other) const;
  Tensor where(const Tensor &amp; condition, const Tensor &amp; other) const;
  Tensor norm(c10::optional&lt;Scalar&gt; p, ScalarType dtype) const;
  Tensor norm(Scalar p=2) const;
  Tensor norm(c10::optional&lt;Scalar&gt; p, IntArrayRef dim, bool keepdim, ScalarType dtype) const;
  Tensor norm(c10::optional&lt;Scalar&gt; p, IntArrayRef dim, bool keepdim=false) const;
  Tensor norm(c10::optional&lt;Scalar&gt; p, DimnameList dim, bool keepdim, ScalarType dtype) const;
  Tensor norm(c10::optional&lt;Scalar&gt; p, DimnameList dim, bool keepdim=false) const;
  Tensor clone(c10::optional&lt;MemoryFormat&gt; memory_format=c10::nullopt) const;
  Tensor &amp; resize_as_(const Tensor &amp; the_template, c10::optional&lt;MemoryFormat&gt; memory_format=c10::nullopt) const;
  Tensor pow(Scalar exponent) const;
  Tensor &amp; zero_() const;
  Tensor sub(const Tensor &amp; other, Scalar alpha=1) const;
  Tensor &amp; sub_(const Tensor &amp; other, Scalar alpha=1) const;
  Tensor sub(Scalar other, Scalar alpha=1) const;
  Tensor &amp; sub_(Scalar other, Scalar alpha=1) const;
  Tensor addmm(const Tensor &amp; mat1, const Tensor &amp; mat2, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; addmm_(const Tensor &amp; mat1, const Tensor &amp; mat2, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; sparse_resize_(IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const;
  Tensor &amp; sparse_resize_and_clear_(IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const;
  Tensor sparse_mask(const Tensor &amp; mask) const;
  Tensor to_dense() const;
  int64_t sparse_dim() const;
  int64_t _dimI() const;
  int64_t dense_dim() const;
  int64_t _dimV() const;
  int64_t _nnz() const;
  Tensor coalesce() const;
  bool is_coalesced() const;
  Tensor _indices() const;
  Tensor _values() const;
  Tensor &amp; _coalesced_(bool coalesced) const;
  Tensor indices() const;
  Tensor values() const;
  std::vector&lt;Tensor&gt; unbind(int64_t dim=0) const;
  std::vector&lt;Tensor&gt; unbind(Dimname dim) const;
  Tensor to_sparse(int64_t sparse_dim) const;
  Tensor to_sparse() const;
  Tensor to_mkldnn() const;
  Tensor dequantize() const;
  double q_scale() const;
  int64_t q_zero_point() const;
  Tensor q_per_channel_scales() const;
  Tensor q_per_channel_zero_points() const;
  int64_t q_per_channel_axis() const;
  Tensor int_repr() const;
  QScheme qscheme() const;
  Tensor to(const TensorOptions &amp; options={}, bool non_blocking=false, bool copy=false, c10::optional&lt;MemoryFormat&gt; memory_format=c10::nullopt) const;
  Tensor to(Device device, ScalarType dtype, bool non_blocking=false, bool copy=false, c10::optional&lt;MemoryFormat&gt; memory_format=c10::nullopt) const;
  Tensor to(ScalarType dtype, bool non_blocking=false, bool copy=false, c10::optional&lt;MemoryFormat&gt; memory_format=c10::nullopt) const;
  Tensor to(const Tensor &amp; other, bool non_blocking=false, bool copy=false, c10::optional&lt;MemoryFormat&gt; memory_format=c10::nullopt) const;
  Scalar item() const;
  Tensor &amp; set_(Storage source) const;
  Tensor &amp; set_(Storage source, int64_t storage_offset, IntArrayRef size, IntArrayRef stride={}) const;
  Tensor &amp; set_(const Tensor &amp; source) const;
  Tensor &amp; set_() const;
  Tensor &amp; set_quantizer_(ConstQuantizerPtr quantizer) const;
  bool is_set_to(const Tensor &amp; tensor) const;
  Tensor &amp; masked_fill_(const Tensor &amp; mask, Scalar value) const;
  Tensor masked_fill(const Tensor &amp; mask, Scalar value) const;
  Tensor &amp; masked_fill_(const Tensor &amp; mask, const Tensor &amp; value) const;
  Tensor masked_fill(const Tensor &amp; mask, const Tensor &amp; value) const;
  Tensor &amp; masked_scatter_(const Tensor &amp; mask, const Tensor &amp; source) const;
  Tensor masked_scatter(const Tensor &amp; mask, const Tensor &amp; source) const;
  Tensor view(IntArrayRef size) const;
  Tensor &amp; put_(const Tensor &amp; index, const Tensor &amp; source, bool accumulate=false) const;
  Tensor &amp; index_add_(int64_t dim, const Tensor &amp; index, const Tensor &amp; source) const;
  Tensor index_add(int64_t dim, const Tensor &amp; index, const Tensor &amp; source) const;
  Tensor index_add(Dimname dim, const Tensor &amp; index, const Tensor &amp; source) const;
  Tensor &amp; index_fill_(int64_t dim, const Tensor &amp; index, Scalar value) const;
  Tensor index_fill(int64_t dim, const Tensor &amp; index, Scalar value) const;
  Tensor &amp; index_fill_(int64_t dim, const Tensor &amp; index, const Tensor &amp; value) const;
  Tensor index_fill(int64_t dim, const Tensor &amp; index, const Tensor &amp; value) const;
  Tensor &amp; index_fill_(Dimname dim, const Tensor &amp; index, Scalar value) const;
  Tensor &amp; index_fill_(Dimname dim, const Tensor &amp; index, const Tensor &amp; value) const;
  Tensor index_fill(Dimname dim, const Tensor &amp; index, Scalar value) const;
  Tensor index_fill(Dimname dim, const Tensor &amp; index, const Tensor &amp; value) const;
  Tensor &amp; scatter_(int64_t dim, const Tensor &amp; index, const Tensor &amp; src) const;
  Tensor scatter(int64_t dim, const Tensor &amp; index, const Tensor &amp; src) const;
  Tensor &amp; scatter_(int64_t dim, const Tensor &amp; index, Scalar value) const;
  Tensor scatter(int64_t dim, const Tensor &amp; index, Scalar value) const;
  Tensor scatter(Dimname dim, const Tensor &amp; index, const Tensor &amp; src) const;
  Tensor scatter(Dimname dim, const Tensor &amp; index, Scalar value) const;
  Tensor &amp; scatter_(int64_t dim, const Tensor &amp; index, const Tensor &amp; src, std::string reduce) const;
  Tensor &amp; scatter_(int64_t dim, const Tensor &amp; index, Scalar value, std::string reduce) const;
  Tensor &amp; scatter_add_(int64_t dim, const Tensor &amp; index, const Tensor &amp; src) const;
  Tensor scatter_add(int64_t dim, const Tensor &amp; index, const Tensor &amp; src) const;
  Tensor scatter_add(Dimname dim, const Tensor &amp; index, const Tensor &amp; src) const;
  Tensor &amp; lt_(Scalar other) const;
  Tensor &amp; lt_(const Tensor &amp; other) const;
  Tensor &amp; gt_(Scalar other) const;
  Tensor &amp; gt_(const Tensor &amp; other) const;
  Tensor &amp; le_(Scalar other) const;
  Tensor &amp; le_(const Tensor &amp; other) const;
  Tensor &amp; ge_(Scalar other) const;
  Tensor &amp; ge_(const Tensor &amp; other) const;
  Tensor &amp; eq_(Scalar other) const;
  Tensor &amp; eq_(const Tensor &amp; other) const;
  Tensor &amp; ne_(Scalar other) const;
  Tensor &amp; ne_(const Tensor &amp; other) const;
  Tensor bitwise_and(Scalar other) const;
  Tensor bitwise_and(const Tensor &amp; other) const;
  Tensor &amp; bitwise_and_(Scalar other) const;
  Tensor &amp; bitwise_and_(const Tensor &amp; other) const;
  Tensor __and__(Scalar other) const;
  Tensor __and__(const Tensor &amp; other) const;
  Tensor &amp; __iand__(Scalar other) const;
  Tensor &amp; __iand__(const Tensor &amp; other) const;
  Tensor bitwise_or(Scalar other) const;
  Tensor bitwise_or(const Tensor &amp; other) const;
  Tensor &amp; bitwise_or_(Scalar other) const;
  Tensor &amp; bitwise_or_(const Tensor &amp; other) const;
  Tensor __or__(Scalar other) const;
  Tensor __or__(const Tensor &amp; other) const;
  Tensor &amp; __ior__(Scalar other) const;
  Tensor &amp; __ior__(const Tensor &amp; other) const;
  Tensor bitwise_xor(Scalar other) const;
  Tensor bitwise_xor(const Tensor &amp; other) const;
  Tensor &amp; bitwise_xor_(Scalar other) const;
  Tensor &amp; bitwise_xor_(const Tensor &amp; other) const;
  Tensor __xor__(Scalar other) const;
  Tensor __xor__(const Tensor &amp; other) const;
  Tensor &amp; __ixor__(Scalar other) const;
  Tensor &amp; __ixor__(const Tensor &amp; other) const;
  Tensor __lshift__(Scalar other) const;
  Tensor __lshift__(const Tensor &amp; other) const;
  Tensor &amp; __ilshift__(Scalar other) const;
  Tensor &amp; __ilshift__(const Tensor &amp; other) const;
  Tensor __rshift__(Scalar other) const;
  Tensor __rshift__(const Tensor &amp; other) const;
  Tensor &amp; __irshift__(Scalar other) const;
  Tensor &amp; __irshift__(const Tensor &amp; other) const;
  Tensor &amp; lgamma_() const;
  Tensor &amp; atan2_(const Tensor &amp; other) const;
  Tensor &amp; tril_(int64_t diagonal=0) const;
  Tensor &amp; triu_(int64_t diagonal=0) const;
  Tensor &amp; digamma_() const;
  Tensor &amp; polygamma_(int64_t n) const;
  Tensor &amp; renorm_(Scalar p, int64_t dim, Scalar maxnorm) const;
  Tensor &amp; pow_(Scalar exponent) const;
  Tensor &amp; pow_(const Tensor &amp; exponent) const;
  Tensor &amp; lerp_(const Tensor &amp; end, Scalar weight) const;
  Tensor &amp; lerp_(const Tensor &amp; end, const Tensor &amp; weight) const;
  Tensor &amp; fmod_(Scalar other) const;
  Tensor &amp; fmod_(const Tensor &amp; other) const;
  Tensor &amp; remainder_(Scalar other) const;
  Tensor &amp; remainder_(const Tensor &amp; other) const;
  Tensor &amp; addbmm_(const Tensor &amp; batch1, const Tensor &amp; batch2, Scalar beta=1, Scalar alpha=1) const;
  Tensor addbmm(const Tensor &amp; batch1, const Tensor &amp; batch2, Scalar beta=1, Scalar alpha=1) const;
  Tensor &amp; addcdiv_(const Tensor &amp; tensor1, const Tensor &amp; tensor2, Scalar value=1) const;
  Tensor &amp; random_(int64_t from, c10::optional&lt;int64_t&gt; to, c10::optional&lt;Generator&gt; generator=c10::nullopt) const;
  Tensor &amp; random_(int64_t to, c10::optional&lt;Generator&gt; generator=c10::nullopt) const;
  Tensor &amp; random_(c10::optional&lt;Generator&gt; generator=c10::nullopt) const;
  Tensor &amp; uniform_(double from=0, double to=1, c10::optional&lt;Generator&gt; generator=c10::nullopt) const;
  Tensor &amp; cauchy_(double median=0, double sigma=1, c10::optional&lt;Generator&gt; generator=c10::nullopt) const;
  Tensor &amp; log_normal_(double mean=1, double std=2, c10::optional&lt;Generator&gt; generator=c10::nullopt) const;
  Tensor &amp; exponential_(double lambd=1, c10::optional&lt;Generator&gt; generator=c10::nullopt) const;
  Tensor &amp; geometric_(double p, c10::optional&lt;Generator&gt; generator=c10::nullopt) const;
  Tensor diag(int64_t diagonal=0) const;
  Tensor cross(const Tensor &amp; other, c10::optional&lt;int64_t&gt; dim=c10::nullopt) const;
  Tensor triu(int64_t diagonal=0) const;
  Tensor tril(int64_t diagonal=0) const;
  Tensor trace() const;
  Tensor ne(Scalar other) const;
  Tensor ne(const Tensor &amp; other) const;
  Tensor eq(Scalar other) const;
  Tensor eq(const Tensor &amp; other) const;
  Tensor ge(Scalar other) const;
  Tensor ge(const Tensor &amp; other) const;
  Tensor le(Scalar other) const;
  Tensor le(const Tensor &amp; other) const;
  Tensor gt(Scalar other) const;
  Tensor gt(const Tensor &amp; other) const;
  Tensor lt(Scalar other) const;
  Tensor lt(const Tensor &amp; other) const;
  Tensor take(const Tensor &amp; index) const;
  Tensor index_select(int64_t dim, const Tensor &amp; index) const;
  Tensor index_select(Dimname dim, const Tensor &amp; index) const;
  Tensor masked_select(const Tensor &amp; mask) const;
  Tensor nonzero() const;
  std::vector&lt;Tensor&gt; nonzero_numpy() const;
  Tensor gather(int64_t dim, const Tensor &amp; index, bool sparse_grad=false) const;
  Tensor gather(Dimname dim, const Tensor &amp; index, bool sparse_grad=false) const;
  Tensor addcmul(const Tensor &amp; tensor1, const Tensor &amp; tensor2, Scalar value=1) const;
  Tensor &amp; addcmul_(const Tensor &amp; tensor1, const Tensor &amp; tensor2, Scalar value=1) const;
  Tensor addcdiv(const Tensor &amp; tensor1, const Tensor &amp; tensor2, Scalar value=1) const;
  std::tuple&lt;Tensor,Tensor&gt; lstsq(const Tensor &amp; A) const;
  std::tuple&lt;Tensor,Tensor&gt; triangular_solve(const Tensor &amp; A, bool upper=true, bool transpose=false, bool unitriangular=false) const;
  std::tuple&lt;Tensor,Tensor&gt; symeig(bool eigenvectors=false, bool upper=true) const;
  std::tuple&lt;Tensor,Tensor&gt; eig(bool eigenvectors=false) const;
  std::tuple&lt;Tensor,Tensor,Tensor&gt; svd(bool some=true, bool compute_uv=true) const;
  Tensor cholesky(bool upper=false) const;
  Tensor cholesky_solve(const Tensor &amp; input2, bool upper=false) const;
  std::tuple&lt;Tensor,Tensor&gt; solve(const Tensor &amp; A) const;
  Tensor cholesky_inverse(bool upper=false) const;
  std::tuple&lt;Tensor,Tensor&gt; qr(bool some=true) const;
  std::tuple&lt;Tensor,Tensor&gt; geqrf() const;
  Tensor orgqr(const Tensor &amp; input2) const;
  Tensor ormqr(const Tensor &amp; input2, const Tensor &amp; input3, bool left=true, bool transpose=false) const;
  Tensor lu_solve(const Tensor &amp; LU_data, const Tensor &amp; LU_pivots) const;
  Tensor multinomial(int64_t num_samples, bool replacement=false, c10::optional&lt;Generator&gt; generator=c10::nullopt) const;
  Tensor lgamma() const;
  Tensor digamma() const;
  Tensor polygamma(int64_t n) const;
  Tensor erfinv() const;
  Tensor &amp; erfinv_() const;
  Tensor sign() const;
  Tensor &amp; sign_() const;
  Tensor signbit() const;
  Tensor dist(const Tensor &amp; other, Scalar p=2) const;
  Tensor atan2(const Tensor &amp; other) const;
  Tensor lerp(const Tensor &amp; end, Scalar weight) const;
  Tensor lerp(const Tensor &amp; end, const Tensor &amp; weight) const;
  Tensor histc(int64_t bins=100, Scalar min=0, Scalar max=0) const;
  Tensor fmod(Scalar other) const;
  Tensor fmod(const Tensor &amp; other) const;
  Tensor hypot(const Tensor &amp; other) const;
  Tensor &amp; hypot_(const Tensor &amp; other) const;
  Tensor nextafter(const Tensor &amp; other) const;
  Tensor &amp; nextafter_(const Tensor &amp; other) const;
  Tensor remainder(Scalar other) const;
  Tensor remainder(const Tensor &amp; other) const;
  Tensor min(const Tensor &amp; other) const;
  Tensor min() const;
  Tensor max(const Tensor &amp; other) const;
  Tensor max() const;
  Tensor median() const;
  Tensor quantile(double q, c10::optional&lt;int64_t&gt; dim=c10::nullopt, bool keepdim=false) const;
  Tensor quantile(const Tensor &amp; q, c10::optional&lt;int64_t&gt; dim=c10::nullopt, bool keepdim=false) const;
  std::tuple&lt;Tensor,Tensor&gt; sort(int64_t dim=-1, bool descending=false) const;
  std::tuple&lt;Tensor,Tensor&gt; sort(Dimname dim, bool descending=false) const;
  Tensor argsort(int64_t dim=-1, bool descending=false) const;
  Tensor argsort(Dimname dim, bool descending=false) const;
  std::tuple&lt;Tensor,Tensor&gt; topk(int64_t k, int64_t dim=-1, bool largest=true, bool sorted=true) const;
  Tensor all() const;
  Tensor any() const;
  Tensor renorm(Scalar p, int64_t dim, Scalar maxnorm) const;
  Tensor unfold(int64_t dimension, int64_t size, int64_t step) const;
  bool equal(const Tensor &amp; other) const;
  Tensor pow(const Tensor &amp; exponent) const;
  Tensor &amp; normal_(double mean=0, double std=1, c10::optional&lt;Generator&gt; generator=c10::nullopt) const;
  Tensor alias() const;
  Tensor isfinite() const;
  Tensor isinf() const;
  Tensor isposinf() const;
  Tensor isneginf() const;
  Tensor fft(int64_t signal_ndim, bool normalized=false) const;
  Tensor det() const;
  Tensor outer(const Tensor &amp; vec2) const;
  Tensor ger(const Tensor &amp; vec2) const;

  // Special C++ only overloads for std()-like functions (See gh-40287)
  // These are needed because int -&gt; bool conversion takes precedence over int -&gt; IntArrayRef
  // So, for example std(0) would select the std(unbiased=False) overload

  Tensor var(int dim) const {
    return var(IntArrayRef{dim});
  }

  Tensor std(int dim) const {
    return std(IntArrayRef{dim});
  }

  // We changed .dtype() to return a TypeMeta in #12766. Ideally, we want the
  // at::kDouble and its friends to be TypeMeta&#39;s, but that hasn&#39;t happened yet.
  // Before that change, we make this method to maintain BC for C++ usage like
  // `x.to(y.dtype)`.
  // TODO: remove following two after at::kDouble and its friends are TypeMeta&#39;s.
  inline Tensor to(caffe2::TypeMeta type_meta, bool non_blocking=false, bool copy=false) const {
    return this-&gt;to(/*scalar_type=*/typeMetaToScalarType(type_meta), non_blocking, copy);
  }
  inline Tensor to(Device device, caffe2::TypeMeta type_meta, bool non_blocking=false, bool copy=false) const {
    return this-&gt;to(device, /*scalar_type=*/typeMetaToScalarType(type_meta), non_blocking, copy);
  }

  template &lt;typename F, typename... Args&gt;
  decltype(auto) m(F func, Args&amp;&amp;... params) const {
    return func(*this, std::forward&lt;Args&gt;(params)...);
  }

  at::Tensor tensor_data() const;

  at::Tensor variable_data() const;

  // Gradient Node and Edges
  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  const std::shared_ptr&lt;torch::autograd::Node&gt;&amp; grad_fn() const;

  // Hooks
  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  template &lt;typename T&gt;
  using hook_return_void_t = std::enable_if_t&lt;std::is_void&lt;typename std::result_of&lt;T&amp;(Tensor)&gt;::type&gt;::value, unsigned&gt;;
  template &lt;typename T&gt;
  using hook_return_var_t = std::enable_if_t&lt;std::is_same&lt;typename std::result_of&lt;T&amp;(Tensor)&gt;::type, Tensor&gt;::value, unsigned&gt;;

  template &lt;typename T&gt;
  hook_return_void_t&lt;T&gt; register_hook(T&amp;&amp; hook) const;
  template &lt;typename T&gt;
  hook_return_var_t&lt;T&gt; register_hook(T&amp;&amp; hook) const;

private:
  unsigned _register_hook(std::function&lt;Tensor(const Tensor&amp;)&gt; hook) const;

public:

  void remove_hook(unsigned pos) const;

  // View Variables
  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  bool is_view() const;

  const Tensor&amp; _base() const;

  // Miscellaneous
  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  const std::string&amp; name() const;

protected:
  friend class ::caffe2::Tensor;

  void enforce_invariants();
  c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt; impl_;
};

int64_t get_device(Tensor self);

template &lt;typename T&gt;
auto Tensor::register_hook(T&amp;&amp; hook) const -&gt; Tensor::hook_return_void_t&lt;T&gt; {
  // Return the grad argument in case of a hook with void return type to have an
  // std::function with Tensor return type
  std::function&lt;void(Tensor)&gt; fn(hook);
  return _register_hook([fn](const Tensor&amp; grad) {
    fn(grad);
    return Tensor();
  });
}

template &lt;typename T&gt;
auto Tensor::register_hook(T&amp;&amp; hook) const -&gt; Tensor::hook_return_var_t&lt;T&gt; {
  return _register_hook(hook);
}

namespace detail {
// Helper creator for Tensor class which doesn&#39;t requires the users to pass
// in an intrusive_ptr instead it just converts the argument passed to
// requested intrusive_ptr type.
template &lt;typename T, typename... Args&gt;
Tensor make_tensor(Args&amp;&amp;... args) {
  return Tensor(c10::make_intrusive&lt;T&gt;(std::forward&lt;Args&gt;(args)...));
}

} // namespace detail

static inline DispatchKey legacyExtractDispatchKey(const Tensor&amp; t) {
  return legacyExtractDispatchKey(t.key_set());
}

} // namespace at
</pre></div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Program Listing for File TensorBody.h</a></li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/language_data.js"></script>
         <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
         <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>