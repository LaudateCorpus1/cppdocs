:github_url: https://github.com/pytorch/pytorch


.. _program_listing_file_torch_csrc_api_include_torch_nn_module.h:

Program Listing for File module.h
=================================

|exhale_lsh| :ref:`Return to documentation for file <file_torch_csrc_api_include_torch_nn_module.h>` (``torch/csrc/api/include/torch/nn/module.h``)

.. |exhale_lsh| unicode:: U+021B0 .. UPWARDS ARROW WITH TIP LEFTWARDS

.. code-block:: none

   #pragma once
   
   #include <torch/detail/ordered_dict.h>
   #include <torch/nn/cursor.h>
   #include <torch/nn/pimpl.h>
   #include <torch/serialize/archive.h>
   #include <torch/tensor.h>
   
   #include <ATen/ATen.h>
   
   #include <map>
   #include <memory>
   #include <string>
   #include <type_traits>
   #include <unordered_map>
   
   // forward declarations confuse doxygen
   #ifndef DOXYGEN_SHOULD_SKIP_THIS
   namespace torch {
   namespace detail {
   template <typename T>
   class CursorBase;
   } // namespace detail
   } // namespace torch
   #endif // DOXYGEN_SHOULD_SKIP_THIS
   
   namespace torch {
   namespace nn {
   
   class Module {
    public:
     explicit Module(std::string name);
   
     Module();
   
     virtual ~Module() = default;
   
     const std::string& name() const noexcept;
   
     virtual std::shared_ptr<Module> clone(
         c10::optional<Device> device = c10::nullopt) const;
   
     ModuleCursor modules();
   
     ConstModuleCursor modules() const;
   
     ModuleCursor children();
   
     ConstModuleCursor children() const;
   
     ParameterCursor parameters();
   
     ConstParameterCursor parameters() const;
   
     BufferCursor buffers();
   
     ConstBufferCursor buffers() const;
   
     virtual void train();
   
     virtual void eval();
   
     virtual bool is_training() const noexcept;
   
     virtual void to(
         torch::Device device,
         torch::Dtype dtype,
         bool non_blocking = false);
   
     virtual void to(torch::Dtype dtype, bool non_blocking = false);
   
     virtual void to(torch::Device device, bool non_blocking = false);
   
     virtual void zero_grad();
   
     template <typename ModuleType>
     typename ModuleType::ContainedType* as() noexcept;
   
     template <
         typename ModuleType,
         typename = torch::detail::disable_if_module_holder_t<ModuleType>>
     ModuleType* as() noexcept;
   
     virtual void save(serialize::OutputArchive& archive) const;
   
     virtual void load(serialize::InputArchive& archive);
   
    protected:
     Tensor& register_parameter(
         std::string name,
         Tensor tensor,
         bool requires_grad = true);
   
     Tensor& register_buffer(std::string name, Tensor tensor);
   
     template <typename ModuleType>
     std::shared_ptr<ModuleType> register_module(
         std::string name,
         std::shared_ptr<ModuleType> module);
   
     template <typename ModuleType>
     std::shared_ptr<ModuleType> register_module(
         std::string name,
         ModuleHolder<ModuleType> module_holder);
   
    private:
     template <typename T>
     using OrderedDict = torch::detail::OrderedDict<std::string, T>;
   
     // Friend classes.
   
     template <typename Derived>
     friend class Cloneable;
     template <typename T>
     friend class detail::CursorBase;
   
     // Private methods.
   
     virtual void clone_(Module& other, c10::optional<Device> device);
   
     template <typename... Ts>
     void to_impl(Ts&&... ts);
   
     OrderedDict<Tensor> parameters_;
   
     OrderedDict<Tensor> buffers_;
   
     OrderedDict<std::shared_ptr<Module>> children_;
   
     mutable c10::optional<std::string> name_;
   
     bool is_training_{true};
   };
   
   // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ nn::Module ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   
   template <typename ModuleType>
   typename ModuleType::ContainedType* Module::as() noexcept {
     // Use the contained type of the `ModuleHolder`, e.g. `LinearImpl` for
     // `Linear`, since `LinearImpl` inherits `nn::Module`.
     return as<typename ModuleType::ContainedType>();
   }
   
   template <typename ModuleType, typename>
   ModuleType* Module::as() noexcept {
     return dynamic_cast<ModuleType*>(this);
   }
   
   template <typename ModuleType>
   std::shared_ptr<ModuleType> Module::register_module(
       std::string name,
       std::shared_ptr<ModuleType> module) {
     AT_CHECK(!name.empty(), "Submodule name must not be empty");
     AT_CHECK(
         name.find('.') == std::string::npos,
         "Submodule name must not contain a dot (got '",
         name,
         "')");
     auto& base_module = children_.insert(std::move(name), std::move(module));
     return std::dynamic_pointer_cast<ModuleType>(base_module);
   }
   
   template <typename ModuleType>
   std::shared_ptr<ModuleType> Module::register_module(
       std::string name,
       ModuleHolder<ModuleType> module_holder) {
     return register_module(std::move(name), module_holder.ptr());
   }
   
   template <typename... Ts>
   void Module::to_impl(Ts&&... ts) {
     // First call `to()` on every child module.
     for (auto& child : children_) {
       child.value->to(ts...);
     }
     // Then move every parameter to the new dtype/device.
     for (auto& parameter : parameters_) {
       parameter->set_data(autograd::Variable(*parameter).data().to(ts...));
     }
     // Then move every buffer to the new dtype/device.
     for (auto& buffer : buffers_) {
       buffer->set_data(autograd::Variable(*buffer).data().to(ts...));
     }
   }
   
   } // namespace nn
   } // namespace torch
