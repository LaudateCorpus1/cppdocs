:github_url: https://github.com/pytorch/pytorch


.. _program_listing_file_torch_csrc_api_include_torch_nn_utils_rnn.h:

Program Listing for File rnn.h
==============================

|exhale_lsh| :ref:`Return to documentation for file <file_torch_csrc_api_include_torch_nn_utils_rnn.h>` (``torch/csrc/api/include/torch/nn/utils/rnn.h``)

.. |exhale_lsh| unicode:: U+021B0 .. UPWARDS ARROW WITH TIP LEFTWARDS

.. code-block:: cpp

   #pragma once
   
   #include <torch/types.h>
   
   namespace torch {
   namespace nn {
   namespace utils {
   namespace rnn {
   
   inline Tensor pad_sequence(
       ArrayRef<Tensor> sequences,
       bool batch_first = false,
       double padding_value = 0) {
     // assuming trailing dimensions and type of all the Tensors
     // in sequences are same and fetching those from sequences[0]
     auto max_size = sequences[0].sizes();
     auto trailing_dims = max_size.slice(1);
     auto max_len = std::max_element(
       sequences.begin(),
       sequences.end(),
       [](const Tensor& a, const Tensor& b) {
         return a.size(0) < b.size(0);
       }
     )->size(0);
   
     std::vector<int64_t> out_dims;
     if (batch_first) {
       out_dims = {(int64_t)sequences.size(), max_len};
     } else {
       out_dims = {max_len, (int64_t)sequences.size()};
     }
     out_dims.insert(out_dims.end(), trailing_dims.begin(), trailing_dims.end());
   
     auto out_tensor = torch::full({out_dims}, padding_value, sequences[0].options());
     for (size_t i = 0; i < sequences.size(); i++) {
       auto tensor = sequences[i];
       int64_t length = tensor.size(0);
       // use index notation to prevent duplicate references to the tensor
       if (batch_first) {
         out_tensor.select(0, i).narrow(0, 0, length).copy_(tensor);
       } else {
         out_tensor.narrow(0, 0, length).select(1, i).copy_(tensor);
       }
     }
     return out_tensor;
   }
   
   } // namespace rnn
   } // namespace utils
   } // namespace nn
   } // namespace torch
