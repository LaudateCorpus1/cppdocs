:github_url: https://github.com/pytorch/pytorch


.. _program_listing_file_torch_csrc_api_include_torch_nn_functional_loss.h:

Program Listing for File loss.h
===============================

|exhale_lsh| :ref:`Return to documentation for file <file_torch_csrc_api_include_torch_nn_functional_loss.h>` (``torch/csrc/api/include/torch/nn/functional/loss.h``)

.. |exhale_lsh| unicode:: U+021B0 .. UPWARDS ARROW WITH TIP LEFTWARDS

.. code-block:: cpp

   #pragma once
   
   #include <torch/nn/options/loss.h>
   
   namespace torch {
   namespace nn {
   namespace functional {
   
   inline Tensor hinge_embedding_loss(
       const Tensor& x1,
       const Tensor& x2,
       const HingeEmbeddingLossOptions& options) {
     return torch::hinge_embedding_loss(
         x1,
         x2,
         options.margin(),
         options.reduction());
   }
   
   inline Tensor multi_margin_loss(
       const Tensor& input,
       const Tensor& target,
       const MultiMarginLossOptions& options = {}) {
     TORCH_CHECK(options.p() == 1 || options.p() == 2, "only p == 1 and p == 2 supported");
     if (options.weight().defined()) {
       TORCH_CHECK(options.weight().dim() == 1, "weight must be one-dimensional");
     }
   
     return torch::multi_margin_loss(
       input,
       target,
       options.p(),
       options.margin(),
       options.weight(),
       options.reduction()
     );
   }
   
   inline Tensor cosine_embedding_loss(
       const Tensor& input1,
       const Tensor& input2,
       const Tensor& target,
       const CosineEmbeddingLossOptions& options) {
     return torch::cosine_embedding_loss(
         input1, input2, target, options.margin(), options.reduction());
   }
   
   } // namespace functional
   } // namespace nn
   } // namespace torch
