:github_url: https://github.com/pytorch/pytorch


.. _program_listing_file_torch_csrc_api_include_torch_nn_options_rnn.h:

Program Listing for File rnn.h
==============================

|exhale_lsh| :ref:`Return to documentation for file <file_torch_csrc_api_include_torch_nn_options_rnn.h>` (``torch/csrc/api/include/torch/nn/options/rnn.h``)

.. |exhale_lsh| unicode:: U+021B0 .. UPWARDS ARROW WITH TIP LEFTWARDS

.. code-block:: cpp

   #pragma once
   
   #include <torch/arg.h>
   #include <torch/csrc/WindowsTorchApiMacro.h>
   #include <torch/types.h>
   
   namespace torch {
   namespace nn {
   
   namespace detail {
   
   struct TORCH_API RNNOptionsBase {
     RNNOptionsBase(int64_t input_size, int64_t hidden_size);
     virtual ~RNNOptionsBase() = default;
     TORCH_ARG(int64_t, input_size);
     TORCH_ARG(int64_t, hidden_size);
     TORCH_ARG(int64_t, layers) = 1;
     TORCH_ARG(bool, with_bias) = true;
     TORCH_ARG(double, dropout) = 0.0;
     TORCH_ARG(bool, bidirectional) = false;
     TORCH_ARG(bool, batch_first) = false;
   };
   
   } // namespace detail
   
   enum class RNNActivation : uint32_t {ReLU, Tanh};
   
   struct TORCH_API RNNOptions {
     RNNOptions(int64_t input_size, int64_t hidden_size);
   
     RNNOptions& tanh();
     RNNOptions& relu();
   
     TORCH_ARG(int64_t, input_size);
     TORCH_ARG(int64_t, hidden_size);
     TORCH_ARG(int64_t, layers) = 1;
     TORCH_ARG(bool, with_bias) = true;
     TORCH_ARG(double, dropout) = 0.0;
     TORCH_ARG(bool, bidirectional) = false;
     TORCH_ARG(bool, batch_first) = false;
     TORCH_ARG(RNNActivation, activation) = RNNActivation::ReLU;
   };
   
   using LSTMOptions = detail::RNNOptionsBase;
   using GRUOptions = detail::RNNOptionsBase;
   
   } // namespace nn
   } // namespace torch
