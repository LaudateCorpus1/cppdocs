:github_url: https://github.com/pytorch/pytorch


.. _program_listing_file_torch_csrc_autograd_variable.h:

Program Listing for File variable.h
===================================

|exhale_lsh| :ref:`Return to documentation for file <file_torch_csrc_autograd_variable.h>` (``torch/csrc/autograd/variable.h``)

.. |exhale_lsh| unicode:: U+021B0 .. UPWARDS ARROW WITH TIP LEFTWARDS

.. code-block:: cpp

   #pragma once
   
   #include <torch/csrc/utils/python_stub.h>
   
   #include <torch/csrc/WindowsTorchApiMacro.h>
   #include <torch/csrc/autograd/edge.h>
   #include <torch/csrc/autograd/function_hook.h>
   #include <torch/csrc/autograd/cpp_hook.h>
   
   #include <ATen/ATen.h>
   #include <ATen/NamedTensorUtils.h>
   #include <c10/util/Exception.h>
   
   #include <memory>
   #include <mutex>
   #include <stdexcept>
   #include <string>
   #include <utility>
   #include <vector>
   #include <cstdint>
   
   namespace torch { namespace autograd {
   
   using Variable = at::Tensor;
   
   } // namespace autograd
   } // namespace torch
   
   // The following are all internal APIs and should not be shown in libtorch docs.
   // Therefore, we wrap the following code with `#ifndef DOXYGEN_SHOULD_SKIP_THIS ... #endif`
   
   #ifndef DOXYGEN_SHOULD_SKIP_THIS
   
   namespace torch { namespace autograd {
   
   struct Node;
   
   
   struct AutogradMeta;
   struct DifferentiableViewMeta;
   
   // Private-ish functions for manipulating variables; we don't want to put them
   // on Tensor proper
   namespace impl {
   
     // WARNING: This may return a nullptr.  If you require AutogradMeta to return
     // a materialized structure, use materialize_autograd_meta instead.
     TORCH_API AutogradMeta* get_autograd_meta(const Variable&);
   
     // Returns the current autograd meta, materializing it if it was previously
     // none.  This counts as a *mutating* operation, so do not call it on
     // "read-only" operators; in particular, this is NOT thread safe
     TORCH_API AutogradMeta* materialize_autograd_meta(const Variable&);
   
     TORCH_API void set_grad_accumulator(const Variable&, std::weak_ptr<Node> grad_accumulator);
   
     TORCH_API std::shared_ptr<Node> try_get_grad_accumulator(const Variable&);
   
     TORCH_API std::shared_ptr<Node> grad_accumulator(const Variable&);
   
     TORCH_API Edge gradient_edge(const Variable&);
   
     TORCH_API void set_gradient_edge(const Variable&, Edge edge);
   
     // Autograd Graph Interaction
     //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   
     TORCH_API void rebase_history(const Variable&, Edge gradient_edge);
   
     TORCH_API Node* grad_fn_unsafe(const Variable&);
   
     TORCH_API void bump_version(const Variable&);
     TORCH_API void set_version_counter(const Variable&, const c10::VariableVersion& version_counter);
   
     TORCH_API const c10::VariableVersion& version_counter(const Variable&);
   
     TORCH_API PyObject* pyobj(const Variable&);
     TORCH_API void set_pyobj(const Variable&, PyObject* pyobj);
   
     TORCH_API void set_name(const Variable&, const std::string& name);
   
     TORCH_API void add_hook(const Variable&, std::shared_ptr<FunctionPreHook> hook);
     TORCH_API const std::vector<std::shared_ptr<FunctionPreHook>>& hooks(const Variable&);
     TORCH_API void clear_hooks(const Variable&);
   
     TORCH_API void create_cpp_hook(const Variable&);
   }
   
   //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   //                            AutogradMeta
   //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   
   
   struct TORCH_API AutogradMeta : public c10::AutogradMetaInterface {
     std::string name_;
   
     Variable grad_;
     std::shared_ptr<Node> grad_fn_;
     std::weak_ptr<Node> grad_accumulator_;
   
     std::vector<std::shared_ptr<FunctionPreHook>> hooks_;
     std::shared_ptr<hooks_list> cpp_hooks_list;
   
     // Only meaningful on leaf variables (must be false otherwise)
     bool requires_grad_;
   
     // Only meaningful on non-leaf variables (must be false otherwise)
     bool retains_grad_;
   
     bool is_view_;
   
     // The "output number" of this variable; e.g., if this variable
     // was the second output of a function, then output_nr == 1.
     // We use this to make sure we can setup the backwards trace
     // correctly when this variable is passed to another function.
     uint32_t output_nr_;
   
     // Mutex to ensure that concurrent read operations that modify internal
     // state are still thread-safe. Used by grad_fn() and
     // grad_accumulator().
     std::mutex mutex_;
   
     void set_requires_grad(bool requires_grad, at::TensorImpl* self_impl) override {
       TORCH_CHECK(
         !requires_grad || at::isFloatingType(at::typeMetaToScalarType(self_impl->dtype())),
         "Only Tensors of floating point dtype can require gradients");
       requires_grad_ = requires_grad;
     }
   
     bool requires_grad() const override {
       return requires_grad_ || grad_fn_;
     }
   
     Variable& grad() override {
       return grad_;
     }
   
     const Variable& grad() const override {
       return grad_;
     }
   
     AutogradMeta(at::TensorImpl* self_impl = nullptr, bool requires_grad = false, Edge gradient_edge = Edge() ) {
       grad_fn_ = std::move(gradient_edge.function);
       requires_grad_ = false;
       retains_grad_ = false;
       is_view_ = false;
       output_nr_ = gradient_edge.input_nr;
   
       // set_requires_grad also checks error conditions.
       if (requires_grad) {
         TORCH_INTERNAL_ASSERT(self_impl);
         set_requires_grad(requires_grad, self_impl);
       }
       TORCH_CHECK(
           !grad_fn_ || !requires_grad_,
           "requires_grad should be false if grad_fn is set");
     }
   };
   
   //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   //                     DifferentiableViewMeta
   //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   
   
   
   
   enum class CreationMeta: uint8_t { DEFAULT, IN_CUSTOM_FUNCTION, MULTI_OUTPUT_NODE,
                                      NO_GRAD_MODE };
   
   TORCH_API void handle_view_on_rebase(DifferentiableViewMeta* diff_view_meta, bool indirect=false);
   
   struct TORCH_API DifferentiableViewMeta : public AutogradMeta {
     Variable base_;
   
     uint32_t attr_version;
   
     c10::optional<std::function<at::Tensor(const at::Tensor&)>> view_fn_;
   
     CreationMeta creation_meta;
   
     bool requires_grad() const override {
       return requires_grad_ || grad_fn_ || (is_view_ && base_.requires_grad());
     }
   
     bool has_view_fn() const {
       return view_fn_.has_value();
     }
   
     std::function<at::Tensor(const at::Tensor&)> view_fn() const {
       TORCH_CHECK(has_view_fn(), "view_fn is not set.");
       return view_fn_.value();
     }
   
     DifferentiableViewMeta(at::TensorImpl* self_impl, Variable base, c10::optional<std::function<at::Tensor(const at::Tensor&)>> view_fn,
                            CreationMeta creation_meta=CreationMeta::DEFAULT);
     ~DifferentiableViewMeta();
   };
   
   //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   //                        Variable Implementation
   //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   
   // Factory Functions
   //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   
   
   
   // See NOTE [ Autograd View Variables ] for details.
   // Differentiable view. Track history with DifferentiableViewMeta.
   inline Variable make_variable_differentiable_view(
       Variable base,
       at::Tensor data,
       CreationMeta creation_meta,
       c10::optional<std::function<at::Tensor(const at::Tensor&)>> view_func = c10::nullopt) {
     if (data.defined()) {
       auto data_impl_copy = data.getIntrusivePtr()->shallow_copy_and_detach(
         /*version_counter=*/0,
         /*allow_tensor_metadata_change=*/true);
       data_impl_copy->set_autograd_meta(std::make_unique<DifferentiableViewMeta>(
         data_impl_copy.get(), std::move(base), std::move(view_func),
         creation_meta));
       return Variable(data_impl_copy);
     }
     return Variable();
   }
   
   // See NOTE [ Autograd View Variables ] for details.
   // Non-differentiable view. Just share version counter.
   inline Variable make_variable_non_differentiable_view(
       Variable base,
       at::Tensor data,
       bool allow_tensor_metadata_change = true) {
     if (data.defined()) {
       auto data_impl_copy = data.getIntrusivePtr()->shallow_copy_and_detach(
         /*version_counter=*/impl::version_counter(base),
         /*allow_tensor_metadata_change=*/allow_tensor_metadata_change);
       data_impl_copy->set_autograd_meta(nullptr);
       return Variable(data_impl_copy);
     }
     return Variable();
   }
   
   inline Variable make_variable(
       at::Tensor data,
       bool requires_grad = false,
       bool allow_tensor_metadata_change = true) {
     if (data.defined()) {
       if (data.getIntrusivePtr().use_count() == 1 && data.getIntrusivePtr()->unique_version()) {
         auto data_impl = data.getIntrusivePtr();
         data_impl->set_allow_tensor_metadata_change(allow_tensor_metadata_change);
         if (requires_grad) {
           data_impl->set_autograd_meta(std::make_unique<AutogradMeta>(data_impl.get(), requires_grad));
         } else {
           data_impl->set_autograd_meta(nullptr);
         }
         return Variable(std::move(data_impl));
       } else {
         auto data_impl_copy = data.getIntrusivePtr()->shallow_copy_and_detach(
           /*version_counter=*/0,
           /*allow_tensor_metadata_change=*/allow_tensor_metadata_change);
         if (requires_grad) {
           data_impl_copy->set_autograd_meta(std::make_unique<AutogradMeta>(
             data_impl_copy.get(), requires_grad));
         } else {
           data_impl_copy->set_autograd_meta(nullptr);
         }
         return Variable(data_impl_copy);
       }
     }
     return Variable();
   }
   
   inline Variable make_variable(
       at::Tensor data,
       Edge gradient_edge,
       bool allow_tensor_metadata_change = true) {
     if (data.defined()) {
       auto data_impl_copy = data.getIntrusivePtr()->shallow_copy_and_detach(
         /*version_counter=*/0,
         /*allow_tensor_metadata_change=*/allow_tensor_metadata_change);
       data_impl_copy->set_autograd_meta(std::make_unique<AutogradMeta>(
         data_impl_copy.get(), false, std::move(gradient_edge)));
       return Variable(data_impl_copy);
     }
     return Variable();
   }
   
   
   }} // namespace torch::autograd
   
   #endif /* DOXYGEN_SHOULD_SKIP_THIS */
